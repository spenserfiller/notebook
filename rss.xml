<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Notebook]]></title><description><![CDATA[A place for thoughts and stuff]]></description><link>https://www.spenserfiller.com</link><generator>GatsbyJS</generator><lastBuildDate>Sun, 12 Oct 2025 15:53:36 GMT</lastBuildDate><item><title><![CDATA[Finding What Feels Good]]></title><description><![CDATA[I love being out on a golf course. There‚Äôs nothing like playing a competitive game with people you enjoy, out in a beautifully manicured bit‚Ä¶]]></description><link>https://www.spenserfiller.com/Golf-and-balance/</link><guid isPermaLink="false">https://www.spenserfiller.com/Golf-and-balance/</guid><pubDate>Sun, 12 Oct 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;I love being out on a golf course. There‚Äôs nothing like playing a competitive game with people you enjoy, out in a beautifully manicured bit of nature. When I‚Äôm playing, there‚Äôs no question this is how I want to spend my time. When I played golf in high school, it was just like the other games I played. I had fun. I also obsessed and could beat all of my friends, but my own natural curiosity, challenge, and fun were the primary motivators.&lt;/p&gt;
&lt;p&gt;Sometime during college and in the workforce, I became more focused on the competitive piece and lost the fun part. This cycle shows itself when playing golf. I start the season out on the course, have a ton of fun, and then start practicing. I can focus on practicing more than I actually play. If I‚Äôm not careful, I stop playing and then eventually lose motivation altogether. The simple fix: return to the course and play a round with people I enjoy. That refills the bar immediately.&lt;/p&gt;
&lt;p&gt;With the right balance, I think I could be a capable golfer. Combining my passion for golf with a little bit of calculated practice would maximize both. That balance is key, though. &lt;/p&gt;
&lt;p&gt;Golf has been a great metaphor for my feelings about tech. I‚Äôve been tinkering and building since I was a kid. It‚Äôs in my DNA. I love taking things apart and learning about how they work. Even when I used this skill productively, I was driven from within. These feelings still have a voice, but it‚Äôs become more challenging as my tech career has grown. It‚Äôs been a while since I‚Äôve truly tinkered with programming and just immersed myself in my own curiosity. It‚Äôs so hard to not get distracted by ‚Äúproductivity.‚Äù I‚Äôve been interested in reinforcement learning for ages, but the golf effect takes over. I start by following that internal drive and next thing I know, I‚Äôm deep in math that my logical brain is telling me will be useful.&lt;/p&gt;
&lt;p&gt;This seems like a core tension in life. How do you follow a path that brings the most fulfillment? Work typically leans toward financial productivity as it‚Äôs difficult to translate these feelings into dollars. I‚Äôve seen this in both self-employed people and those working in large corporations. It seems like a natural force that one must navigate in addition to their own internal force. When this light shines at work, it‚Äôs often dampened by bureaucracy and conflicts of interest. It‚Äôs rare that I get to genuinely focus and let the creativity flow.&lt;/p&gt;
&lt;p&gt;So that‚Äôs what I‚Äôm focusing on right now. How do I fill that cup up? You must feed your creativity to strengthen it enough to resist the core drive to be productive. Perhaps building and cultivating a space filled with my ideas and experiences will work those muscles and tend the flame. Ultimately, it‚Äôs for our own good; it is productive. I know there are things that inspire me about tech, but how do I just dive in without trying to squeeze financial productivity out of it? How do I rekindle that fire for building cool shit?&lt;/p&gt;</content:encoded></item><item><title><![CDATA[October 2025]]></title><description><![CDATA[Introduction to a Self Managed Life: a 13 hour & 28 minute presentation by FUTO software - Cool resource that Jacob shared. It goes into the‚Ä¶]]></description><link>https://www.spenserfiller.com/10-7-2025/</link><guid isPermaLink="false">https://www.spenserfiller.com/10-7-2025/</guid><pubDate>Tue, 07 Oct 2025 00:00:00 GMT</pubDate><content:encoded>&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://wiki.futo.org/index.php/Introduction_to_a_Self_Managed_Life:_a_13_hour_%26_28_minute_presentation_by_FUTO_software&quot;&gt;Introduction to a Self Managed Life: a 13 hour &amp;#x26; 28 minute presentation by FUTO software&lt;/a&gt; - Cool resource that Jacob shared. It goes into the depths of DIY networking related tools. Routers, own server, self managed email.. etc. Homesteading for computers&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://openlaunch.io/products/nova-golf-entertainment-system&quot;&gt;Open data golf launch monitor&lt;/a&gt; - I thought this was completely open source but I believe just the software/data is? Or maybe just the data? It‚Äôs pretty cool, tho. You can control it with club movement which is pretty noval in the space.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://puffer.ai/&quot;&gt;Puffer AI - RL learning tool&lt;/a&gt; - ‚ÄúPufferLib is a fast and sane reinforcement learning library that can train tiny, super-human models in seconds.‚Äù I am thinking of using this as a start to get back into RL. It‚Äôs kind of openAI gym combined with RL tools for easy exploration. Seems pretty cool. &lt;a href=&quot;https://github.com/pufferai/pufferlib&quot;&gt;Github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://youtu.be/Bdj14_jdumI?si=n0r_yXVQcHyq2cyB&quot;&gt;be your own algorithm&lt;/a&gt; - This was pretty interesting. This was largely a discussion around the book ‚ÄúFilterWorld‚Äù. It sounds like the author had many views that I‚Äôve shared in the past. It felt like she was addressing me to some extent. I‚Äôve been interested in the idea of be your own algorithm and resonate with much of her arguments. I might make this a longer post, but tldr: social media and content creators aren‚Äôt inherently bad. There‚Äôs tons of good stuff, better than ever before. It‚Äôs the platforms that are toxic. The internet is still an amazing place.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://static1.squarespace.com/static/5838a24729687f08e0321a15/t/5bf2bdfa562fa782871c6252/1542635003373/The-Paper-Menagerie+by+Ken+Liu.pdf&quot;&gt;The Paper Menagerie - Ken Liu&lt;/a&gt; - This was mentioned by the next link ‚Äúbe your own algorithm‚Äù. I haven‚Äôt read it but it‚Äôs a pretty short story about ai.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.amazon.com/Filterworld-How-Algorithms-Flattened-Culture/dp/0385548281&quot;&gt;FilterWorld - Kyle Chayka&lt;/a&gt; - This was the focus on ‚Äúbe your own algorithm‚Äù. Not sure I want to read based on the video, but seems interesting for discussions around social media, the internet, and culture&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://theinternetisshit.xyz/&quot;&gt;The internet is shit&lt;/a&gt; - a website that hosts a bunch of other small ‚Äútasteful‚Äù websites. &lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.reddit.com/r/indieweb/&quot;&gt;The Indie Web&lt;/a&gt; - This has been a reoccuring theme in my internet usage lately. Trying to stay off of the major platforms (facebook, instagram, x, etc..). My desire to build out my personal site has been largely based on this.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.recurse.com/&quot;&gt;The Recurse Center&lt;/a&gt; - This was shared by Jacob as well. Kind of like a coding bootcamp but you work on whatever you want. Sort of a guild of people who want social reinforcement for extended technical sabbaticals.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Bhagavad_Gita&quot;&gt;Bhagavad Gita&lt;/a&gt; - The Bhagavad Gita is a 700-verse Hindu scripture that forms part of the Indian epic Mahabharata. Presented as a dialogue between Prince Arjuna and the god Krishna, who serves as his charioteer, it explores profound themes of duty (dharma), righteous action, devotion, and self-realization. The Gita is revered as a spiritual and philosophical guide, offering insights into how to live a life of purpose, balance, and inner peace amid moral and emotional conflict.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://randsinrepose.com/archives/bored-people-quit/&quot;&gt;Why Bored People Quit&lt;/a&gt; - A short article JC shared with me about how managers should help prevent burnout. Being bored isn‚Äôt helpful for anyone. I am definitely bored at work.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://roadmap.sh/roadmaps&quot;&gt;Developer Roadmaps&lt;/a&gt; - A massive project for providing various roadmaps to different technical tracks. I am interested in this for RL, but also the concept as a whole. I want to add another tab to this blog for my own skill-tree/track. Help give some visual representation for what I‚Äôm moving toward.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.destroyallsoftware.com/&quot;&gt;Destroy All Software&lt;/a&gt; - A service that Jacob shared that aims to teach the core of software and computers. Understand it all.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Determined:_A_Science_of_Life_Without_Free_Will&quot;&gt;Determined - Sapolsky&lt;/a&gt; - I‚Äôm currently reading Behave by Sapolsky and this is another interesting one that I haven‚Äôt read. Seems to be a biological approach to why we make decisions.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://hackaday.io/project/195042/logs&quot;&gt;PiTrac Golf Launch Monitor&lt;/a&gt; - A completely open source launch monitor. Not sure it‚Äôs worth the effort or not, but very cool. Could be a fun electronics project at some point. &lt;a href=&quot;https://github.com/PiTracLM/PiTrac&quot;&gt;Github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/The_Lies_of_Locke_Lamora&quot;&gt;Lies of Locked Lamora&lt;/a&gt; - Shane said one of his facorite books. He also mentioned &lt;a href=&quot;https://en.wikipedia.org/wiki/Foundryside&quot;&gt;Foundryside&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/A_Drop_of_Corruption&quot;&gt;A Drop of Corruption&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://huggingface.co/IIEleven11/Kalypso&quot;&gt;Kalypso llm&lt;/a&gt; - A llm model with no guardrails. Praised for good story telling.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://geoffreyducharme.substack.com/&quot;&gt;Sophia - Geoffrey&lt;/a&gt; - Some articles about the system written by one of my previous managers.&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[How Stress, Posture, and Breathing Impact Ulcerative Colitis]]></title><description><![CDATA[Understanding Ulcerative Colitis and the Role of Stress and Inflammation Ulcerative Colitis (UC) is a common inflammatory bowel disease (IBD‚Ä¶]]></description><link>https://www.spenserfiller.com/stress-posture-breathing-uc/</link><guid isPermaLink="false">https://www.spenserfiller.com/stress-posture-breathing-uc/</guid><pubDate>Sun, 09 Mar 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h4&gt;Understanding Ulcerative Colitis and the Role of Stress and Inflammation&lt;/h4&gt;
&lt;p&gt;Ulcerative Colitis (UC) is a common inflammatory bowel disease (IBD) affecting the colon and rectum, with symptoms like bloody diarrhea, abdominal cramps, and fatigue that fluctuate between remission and flare-ups. While stress doesn‚Äôt cause UC, research shows it can worsen symptoms, making stress management vital. Inflammation drives UC, and reducing it can improve quality of life. Breathing-focused practices engage the parasympathetic nervous system (PNS)‚Äîthe ‚Äúrest and digest‚Äù system‚Äîcounteracting stress-induced inflammation.&lt;/p&gt;
&lt;h4&gt;The Parasympathetic Nervous System (PNS) Explained&lt;/h4&gt;
&lt;p&gt;The PNS calms the body by slowing heart rate, lowering blood pressure, enhancing digestion, and reducing cortisol, which helps lower inflammation‚Äîa key factor in UC. Activating the PNS counters the ‚Äúfight or flight‚Äù sympathetic nervous system (SNS), promoting relaxation and healing.&lt;/p&gt;
&lt;h4&gt;How Mental Work and Posture Affect UC&lt;/h4&gt;
&lt;p&gt;Intense mental focus, even without overt stress, can subtly activate the SNS by increasing cognitive load, raising stress hormones over time, and contributing to inflammation. For UC, this may worsen symptoms. Similarly, poor posture from prolonged sitting reduces lung capacity by up to 30%, limiting diaphragm movement and causing shallow breathing, which increases tension and inflammation (&lt;a href=&quot;https://www.airphysiotherapy.co.uk/poor-posture-breathing-pattern-disorder/&quot;&gt;Air Physiotherapy&lt;/a&gt;, &lt;a href=&quot;https://pmc.ncbi.nlm.nih.gov/articles/PMC4756000/&quot;&gt;Journal of Physical Therapy Science&lt;/a&gt;).&lt;/p&gt;
&lt;h4&gt;Wim Hof Breathing and Cold Showers&lt;/h4&gt;
&lt;p&gt;The Wim Hof method pairs breathing techniques with cold exposure to reduce inflammation. A 2024 &lt;em&gt;PLOS ONE&lt;/em&gt; review found it boosts anti-inflammatory IL-10 and lowers pro-inflammatory cytokines like TNF-Œ± (&lt;a href=&quot;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0286933&quot;&gt;PLOS ONE: Wim Hof Method and Inflammation&lt;/a&gt;). A 2014 study showed reduced inflammatory responses, suggesting benefits for UC (&lt;a href=&quot;https://www.pnas.org/doi/10.1073/pnas.1322174111&quot;&gt;PNAS&lt;/a&gt;). Cold showers may also help, with evidence supporting stress resistance (&lt;a href=&quot;https://www.healthline.com/health/cold-water-therapy&quot;&gt;Healthline&lt;/a&gt;).&lt;/p&gt;
&lt;h4&gt;Practices to Activate the PNS&lt;/h4&gt;
&lt;p&gt;Here‚Äôs a concise list of practices to engage the PNS and manage UC:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Practice&lt;/th&gt;
&lt;th&gt;Benefit&lt;/th&gt;
&lt;th&gt;Mechanism&lt;/th&gt;
&lt;th&gt;Research&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Wim Hof Breathing&lt;/td&gt;
&lt;td&gt;Reduces inflammation, stress&lt;/td&gt;
&lt;td&gt;Boosts anti-inflammatory cytokines&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0286933&quot;&gt;PLOS ONE: Wim Hof Method and Inflammation&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cold Showers&lt;/td&gt;
&lt;td&gt;Potential anti-inflammatory effects&lt;/td&gt;
&lt;td&gt;May lower pro-inflammatory markers&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://www.healthline.com/health/cold-water-therapy&quot;&gt;Healthline&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Meditation&lt;/td&gt;
&lt;td&gt;Lowers stress, inflammatory biomarkers&lt;/td&gt;
&lt;td&gt;Enhances PNS, reduces cortisol&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://www.nature.com/articles/s41598-020-63168-4&quot;&gt;Scientific Reports&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Yoga for Posture&lt;/td&gt;
&lt;td&gt;Improves stress, digestion, posture&lt;/td&gt;
&lt;td&gt;Enhances breathing, reduces inflammation&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0022399919309195&quot;&gt;Journal of Clinical Medicine&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mindfulness Walking&lt;/td&gt;
&lt;td&gt;Reduces stress, boosts clarity&lt;/td&gt;
&lt;td&gt;Promotes relaxation&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://www.frontiersin.org/articles/10.3389/fpsyt.2022.797701/full&quot;&gt;Frontiers in Psychiatry&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tai Chi&lt;/td&gt;
&lt;td&gt;Reduces stress, may lower inflammation&lt;/td&gt;
&lt;td&gt;Combines movement and meditation&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://www.healthline.com/health/tai-chi-benefits&quot;&gt;Healthline&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Progressive Muscle Relaxation&lt;/td&gt;
&lt;td&gt;Eases stress, muscle tension&lt;/td&gt;
&lt;td&gt;Promotes relaxation&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://www.mayoclinic.org/healthy-lifestyle/stress-management/in-depth/relaxation-technique/art-20045368&quot;&gt;Mayo Clinic&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Regular Physical Activity&lt;/td&gt;
&lt;td&gt;Boosts mood, reduces inflammation&lt;/td&gt;
&lt;td&gt;Increases endorphins&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://www.webmd.com/ibd-crohns-disease/ulcerative-colitis/uc-exercise&quot;&gt;WebMD&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Good Sleep Hygiene&lt;/td&gt;
&lt;td&gt;Manages stress, inflammation&lt;/td&gt;
&lt;td&gt;Improves recovery, lowers cortisol&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://www.health.harvard.edu/healthbeat/how-sleep-deprivation-can-cause-inflammation&quot;&gt;Harvard Medical&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Anti-Inflammatory Diet&lt;/td&gt;
&lt;td&gt;Supports symptom management&lt;/td&gt;
&lt;td&gt;Provides anti-inflammatory nutrients&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://www.healthline.com/nutrition/anti-inflammatory-diet-101&quot;&gt;Healthline&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Notable Additions:&lt;/strong&gt; Try body scan meditation, Pilates for yoga, or qigong with mindfulness walking.&lt;/p&gt;
&lt;h4&gt;Deep Breathing Exercises&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Diaphragmatic Breathing:&lt;/strong&gt; Inhale through your nose for 4, expanding your belly, exhale for 6. Repeat 5-10 times.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Box Breathing:&lt;/strong&gt; Inhale, hold, exhale, and hold again‚Äîeach for 4 counts (4-4-4-4). Repeat 4-5 cycles.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;4-7-8 Breathing:&lt;/strong&gt; Inhale for 4, hold for 7, exhale for 8. Do 4-5 rounds.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Mindful Walking Practices&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Basic:&lt;/strong&gt; Walk slowly, noticing each step and all of the sensations in your body for 5-10 minutes.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sensory-Focused:&lt;/strong&gt; Tune into sights, sounds, and smells for 10-15 minutes.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Breath-Synchronized:&lt;/strong&gt; Match breaths to steps (e.g., inhale 3 steps, exhale 3) for 5-10 minutes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Mindfulness Dog Walking&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Breath Awareness:&lt;/strong&gt; Focus on steady breathing while walking; take deep breaths when your dog stops.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stop-and-Breathe:&lt;/strong&gt; Use pauses to inhale and observe your surroundings, exhaling tension.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Exercise and PNS Balance&lt;/h4&gt;
&lt;p&gt;Intense exercise activates the SNS but engages the PNS afterward, aiding recovery. However, jumping from exercise to mentally taxing work can overstimulate the SNS. Use PNS practices like deep breathing or meditation post-exercise and during/after work for a smoother transition.&lt;/p&gt;
&lt;hr&gt;
&lt;h4&gt;Summary: Connecting the Dots&lt;/h4&gt;
&lt;p&gt;I‚Äôve been gathering these insights for years, noticing a link between inflammation and my lifestyle, even though extreme UC symptoms are rare for me. I‚Äôve long suspected these elements tie together and though I didn‚Äôt know about the autonomic nervous system (ANS) before, the evidence suggests it‚Äôs central to my experience. My racing mind rarely slows unless I‚Äôm intentional and working at a desk 8+ hours daily amplifies this, requiring extra effort. My brain goes into overdrive, spilling into other areas of life. Despite feeling positive, I think my nervous system is worn out, not letting my PNS do its job. Exercise has taught me a lot about this balance, shaping my lifestyle goals. The good news? These practices noticeably improve my experience, benefiting other areas too, though they take time. Subconsciously, I‚Äôve been drawn to these ideas ‚Äî maybe my gut-brain connection is actually steering my goals! This journey has solidified my belief in resting and recovering, even when I don‚Äôt feel ‚Äútired‚Äù like one does after exercise. Thinking all day might be like walking all day: you may not feel exhausted, but your body still needs to heal. &lt;/p&gt;
&lt;p&gt;So give your mind some space, let go, focus on your breath, and let your body do what it knows best.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[How GPT Models Generate Text: A Step-by-Step Breakdown]]></title><description><![CDATA[Understanding Text Generation in GPT Models: A Deep Dive In this post, we‚Äôll explore how GPT models generate text step-by-step. By decoding‚Ä¶]]></description><link>https://www.spenserfiller.com/fourth-post/</link><guid isPermaLink="false">https://www.spenserfiller.com/fourth-post/</guid><pubDate>Tue, 04 Feb 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;Understanding Text Generation in GPT Models: A Deep Dive&lt;/h1&gt;
&lt;p&gt;In this post, we‚Äôll explore how GPT models generate text step-by-step. By decoding and breaking down how tokens are processed, passed through the model, and mapped back to vocabulary, we‚Äôll answer some key questions about how GPT works under the hood.&lt;/p&gt;
&lt;h2&gt;How GPT Generates Text&lt;/h2&gt;
&lt;p&gt;Text generation in GPT models is an &lt;strong&gt;autoregressive process&lt;/strong&gt;, which means the model generates one token at a time, based on previously generated tokens. Here‚Äôs a simplified function for greedy text generation:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;generate_text_simple&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;model&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; idx&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; max_new_tokens&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; context_size&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; _ &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;max_new_tokens&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;token comment&quot;&gt;# Trim the input to fit the model&apos;s context window&lt;/span&gt;
        idx_cond &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; idx&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;context_size&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;
        
        &lt;span class=&quot;token comment&quot;&gt;# Pass through the model (disable gradient calculation)&lt;/span&gt;
        &lt;span class=&quot;token keyword&quot;&gt;with&lt;/span&gt; torch&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;no_grad&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
            logits &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; model&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;idx_cond&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;token comment&quot;&gt;# Focus on the last time step&lt;/span&gt;
        logits &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; logits&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;  &lt;span class=&quot;token comment&quot;&gt;# Shape: (batch_size, vocab_size)&lt;/span&gt;
        
        &lt;span class=&quot;token comment&quot;&gt;# Convert logits to probabilities&lt;/span&gt;
        probas &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; torch&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;softmax&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;logits&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; dim&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;token comment&quot;&gt;# Pick the most likely next token (greedy decoding)&lt;/span&gt;
        idx_next &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; torch&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;argmax&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;probas&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; dim&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; keepdim&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;token comment&quot;&gt;# Append the new token to the input sequence&lt;/span&gt;
        idx &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; torch&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;cat&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;idx&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; idx_next&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; dim&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; idx&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;What‚Äôs Happening Here?&lt;/h3&gt;
&lt;p&gt;This function generates new tokens one by one using &lt;strong&gt;greedy decoding&lt;/strong&gt;, where we always choose the most likely token at each step. Let‚Äôs break it down further:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Trimming the Input Context:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;idx_cond &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; idx&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;context_size&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;GPT has a fixed context window. If the input sequence exceeds this size, we keep only the most recent tokens.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Passing the Input Through the Model:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;logits &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; model&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;idx_cond&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The model processes the input and outputs a set of &lt;strong&gt;logits&lt;/strong&gt; for each token in the input sequence. Logits are raw scores that we‚Äôll later convert to probabilities.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Extracting the Last Token‚Äôs Logits:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;logits &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; logits&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;  &lt;span class=&quot;token comment&quot;&gt;# Shape: (batch_size, vocab_size)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Since we‚Äôre generating text autoregressively (one token at a time), we only care about the logits for the &lt;strong&gt;last time step&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Converting Logits to Probabilities:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;probas &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; torch&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;softmax&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;logits&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; dim&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The softmax function normalizes the logits into a &lt;strong&gt;probability distribution&lt;/strong&gt; over the entire vocabulary.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Selecting the Most Likely Token:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;idx_next &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; torch&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;argmax&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;probas&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; dim&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; keepdim&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This line picks the token with the highest probability (greedy decoding). The token ID is appended to the input sequence.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2&gt;Tokens, Embeddings, and Vocabulary: How It All Fits Together&lt;/h2&gt;
&lt;h3&gt;Step 1: From Token to Embedding&lt;/h3&gt;
&lt;p&gt;When we feed text into GPT, it‚Äôs first tokenized into subword units (e.g., ‚ÄúHello world!‚Äù ‚Üí &lt;code class=&quot;language-text&quot;&gt;[620, 1567, 198]&lt;/code&gt;), and then these token IDs are mapped to &lt;strong&gt;embeddings&lt;/strong&gt;. Each embedding is a dense vector of size &lt;code class=&quot;language-text&quot;&gt;d_model&lt;/code&gt; (e.g., 768 for GPT-2 small).&lt;/p&gt;
&lt;h3&gt;Step 2: Passing Embeddings Through Transformer Layers&lt;/h3&gt;
&lt;p&gt;The embeddings are passed through multiple transformer layers, which apply &lt;strong&gt;self-attention&lt;/strong&gt;, &lt;strong&gt;feedforward transformations&lt;/strong&gt;, and &lt;strong&gt;normalization&lt;/strong&gt;. This enriches each token‚Äôs embedding with contextual information from the entire input sequence.&lt;/p&gt;
&lt;h3&gt;Step 3: Mapping Hidden States to Logits&lt;/h3&gt;
&lt;p&gt;After the final transformer layer, the hidden states are mapped to a &lt;strong&gt;logits vector&lt;/strong&gt; of size &lt;code class=&quot;language-text&quot;&gt;vocab_size&lt;/code&gt;. Each element in this vector represents the model‚Äôs score for a particular token in the vocabulary.&lt;/p&gt;
&lt;h3&gt;Step 4: Converting Logits to Tokens&lt;/h3&gt;
&lt;p&gt;To generate the next token:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Softmax&lt;/strong&gt; converts logits to probabilities.&lt;/li&gt;
&lt;li&gt;We pick the most probable token (using greedy decoding, or alternatively, sampling techniques like top-k or nucleus sampling).&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2&gt;Why Do We Pick a Token Instead of Directly Converting the Embedding?&lt;/h2&gt;
&lt;p&gt;This is a key insight! The model doesn‚Äôt directly output a token‚Äîit outputs a high-dimensional vector (logits) with scores for each token in the vocabulary. By selecting the token with the highest score, we pick the &lt;strong&gt;closest match&lt;/strong&gt; in terms of meaning and context.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Understanding the Vocabulary and Decoding Process&lt;/h2&gt;
&lt;p&gt;The vocabulary is implicitly referenced throughout the process:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Input tokens (&lt;code class=&quot;language-text&quot;&gt;idx&lt;/code&gt;)&lt;/strong&gt; are indices that map to specific vocabulary tokens.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model outputs logits&lt;/strong&gt; that represent scores for every token in the vocabulary.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Final decoding step&lt;/strong&gt; converts logits to token IDs, which can then be converted back to text using a tokenizer.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;Visualization: Putting It All Together&lt;/h2&gt;
&lt;p&gt;Here‚Äôs a quick visualization of the key steps:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;Input Text ‚Üí Token IDs ‚Üí Embeddings ‚Üí Transformer Layers ‚Üí Logits ‚Üí Probabilities ‚Üí Next Token&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;hr&gt;
&lt;h2&gt;Key Takeaways&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;GPT generates text autoregressively&lt;/strong&gt;, one token at a time.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embeddings are processed through transformer layers&lt;/strong&gt;, which enrich them with contextual meaning.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Logits are mapped to tokens using softmax and decoding methods&lt;/strong&gt; (like greedy decoding).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The vocabulary is referenced implicitly through token IDs&lt;/strong&gt;, logits, and final decoding steps.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;By understanding this flow, you‚Äôll gain a deeper appreciation for how GPT models generate coherent and contextually rich text. Whether you‚Äôre building your own transformer-based model or fine-tuning GPT, these fundamentals are essential.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Next Steps&lt;/h2&gt;
&lt;p&gt;Want to experiment? Try modifying the text generation function to use &lt;strong&gt;top-k sampling&lt;/strong&gt;, &lt;strong&gt;temperature scaling&lt;/strong&gt;, or &lt;strong&gt;beam search&lt;/strong&gt; to explore different decoding strategies!&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Why Do Neural Networks Need Activation Functions?]]></title><description><![CDATA[Understanding Why Depth Needs Non-Linearity Deep learning models are built from layers of neurons, but one of the most important ingredients‚Ä¶]]></description><link>https://www.spenserfiller.com/third-post/</link><guid isPermaLink="false">https://www.spenserfiller.com/third-post/</guid><pubDate>Fri, 31 Jan 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;Understanding Why Depth Needs Non-Linearity&lt;/h2&gt;
&lt;p&gt;Deep learning models are built from layers of neurons, but one of the most important ingredients that makes them work effectively is the activation function.&lt;/p&gt;
&lt;p&gt;If you‚Äôve ever wondered:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Why can‚Äôt we just stack many linear layers?&lt;/li&gt;
&lt;li&gt;What does an activation function actually do?&lt;/li&gt;
&lt;li&gt;How does setting some values to 0 help learning?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This post will break it all down in an intuitive way.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;1. What Happens Without Activation Functions?&lt;/h2&gt;
&lt;p&gt;At their core, neural networks perform a series of mathematical transformations on input data. A simple linear layer applies the transformation:&lt;/p&gt;
&lt;p&gt;[ y = Wx + b ]&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;( W ) is a weight matrix that scales inputs,&lt;/li&gt;
&lt;li&gt;( x ) is the input vector (e.g., an image, text embedding, etc.),&lt;/li&gt;
&lt;li&gt;( b ) is a bias term that shifts the output.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we stack multiple linear layers, you might expect the network to learn more complex representations. However, something surprising happens:&lt;/p&gt;
&lt;p&gt;[ h&lt;em&gt;1 = W&lt;/em&gt;1 x + b&lt;em&gt;1 ]
[ h&lt;/em&gt;2 = W&lt;em&gt;2 h&lt;/em&gt;1 + b_2 ]&lt;/p&gt;
&lt;p&gt;Substituting ( h_1 ):&lt;/p&gt;
&lt;p&gt;[ h&lt;em&gt;2 = W&lt;/em&gt;2 (W&lt;em&gt;1 x + b&lt;/em&gt;1) + b&lt;em&gt;2 ]
[ h&lt;/em&gt;2 = (W&lt;em&gt;2 W&lt;/em&gt;1) x + (W&lt;em&gt;2 b&lt;/em&gt;1 + b_2) ]&lt;/p&gt;
&lt;p&gt;This is still just one big linear function! No matter how many layers we stack, we never get the ability to model complex, non-linear patterns.&lt;/p&gt;
&lt;p&gt;This means:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The network can only separate data using straight lines (or planes in higher dimensions).&lt;/li&gt;
&lt;li&gt;It cannot adapt to curved decision boundaries needed for real-world problems.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;2. How Activation Functions Break This Limitation&lt;/h2&gt;
&lt;p&gt;To make neural networks truly powerful, we need to introduce non-linearity. This is where activation functions come in.&lt;/p&gt;
&lt;p&gt;When we apply an activation function ( f(x) ) after each layer:&lt;/p&gt;
&lt;p&gt;[ h&lt;em&gt;1 = f(W&lt;/em&gt;1 x + b&lt;em&gt;1) ]
[ h&lt;/em&gt;2 = f(W&lt;em&gt;2 h&lt;/em&gt;1 + b_2) ]&lt;/p&gt;
&lt;p&gt;Now, we can‚Äôt simplify the layers into one big equation‚Äîeach layer transforms the data differently based on the activation function, allowing the network to model complex relationships.&lt;/p&gt;
&lt;p&gt;But why does this work?&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;3. How Does an Activation Function ‚ÄúBreak It Up‚Äù?&lt;/h2&gt;
&lt;p&gt;The key to activation functions is that they change how information flows between layers.&lt;/p&gt;
&lt;p&gt;Without activation functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each layer predictably transforms the input in a fixed, linear way.&lt;/li&gt;
&lt;li&gt;The next layer can always adjust for the previous layer‚Äôs transformation, meaning the network behaves like one big layer.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With activation functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Certain values get modified (or set to zero).&lt;/li&gt;
&lt;li&gt;This forces the next layer to adapt in different ways depending on the input.&lt;/li&gt;
&lt;li&gt;Instead of a single, predictable transformation, the network now learns piecewise, non-linear functions that approximate curves.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example: ReLU Activation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;[ \text{ReLU}(x) = \max(0, x) ]&lt;/p&gt;
&lt;p&gt;If ( x ) is negative, it becomes 0 (neuron turns off). If ( x ) is positive, it stays the same (neuron passes information forward).&lt;/p&gt;
&lt;p&gt;This breaks linearity because some neurons completely stop contributing in certain areas of the input space. The next layer must now adapt based on which neurons are active, rather than always seeing the same structured transformation.&lt;/p&gt;
&lt;p&gt;This is what lets deep networks approximate curves instead of just drawing straight lines.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;4. Why ReLU or GELU?&lt;/h2&gt;
&lt;p&gt;ReLU (Rectified Linear Unit) is the most common activation function because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It‚Äôs simple and fast (just a max operation).&lt;/li&gt;
&lt;li&gt;It prevents vanishing gradients (unlike Sigmoid and Tanh).&lt;/li&gt;
&lt;li&gt;It creates sparse activations, helping the model learn more efficiently.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, ReLU has a problem: some neurons ‚Äúdie‚Äù and never activate (if they always receive negative inputs).&lt;/p&gt;
&lt;p&gt;GELU (Gaussian Error Linear Unit) is a smoother alternative used in models like Transformers (GPT, BERT). It behaves like ReLU for large values but gradually scales small negative values instead of cutting them off completely. This leads to better training stability in deep models.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use ReLU for most applications (CNNs, general deep learning).&lt;/li&gt;
&lt;li&gt;Use GELU for deep architectures where smooth activation helps, like NLP models.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;5. Final Thoughts: Activation Functions Are the Key to Depth&lt;/h2&gt;
&lt;p&gt;If there‚Äôs one thing to take away from this post, it‚Äôs this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Without activation functions, neural networks are just big linear equations.&lt;/li&gt;
&lt;li&gt;Activation functions ‚Äúbreak up‚Äù predictability, enabling deep networks to learn complex patterns.&lt;/li&gt;
&lt;li&gt;ReLU is simple and fast; GELU is smoother and better for deep architectures.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Deep learning isn‚Äôt just about stacking layers‚Äîit‚Äôs about stacking them in a way that introduces the right kinds of transformations. Activation functions are the secret sauce that make deep learning truly deep.&lt;/p&gt;
&lt;hr&gt;
&lt;h3&gt;Want to Go Deeper?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Try experimenting with activation functions in PyTorch or TensorFlow.&lt;/li&gt;
&lt;li&gt;Look at how ReLU and GELU behave visually.&lt;/li&gt;
&lt;li&gt;Read more about how activation functions impact learning dynamics.&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[The AI That Trains Itself: How DeepSeek-R1 Could Change Everything]]></title><description><![CDATA[For years, AI training has relied on human-guided alignment, where human raters score responses, and models are fine-tuned based on their‚Ä¶]]></description><link>https://www.spenserfiller.com/second-post/deepseek-r1-blog/</link><guid isPermaLink="false">https://www.spenserfiller.com/second-post/deepseek-r1-blog/</guid><pubDate>Wed, 29 Jan 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;For years, AI training has relied on human-guided alignment, where human raters score responses, and models are fine-tuned based on their preferences. But DeepSeek-R1 has introduced something radically different‚Äîan AI that teaches itself, refining its reasoning without human oversight or another AI acting as a judge.&lt;/p&gt;
&lt;p&gt;This isn‚Äôt just an optimization; it‚Äôs a fundamental shift in AI learning. Instead of following human opinions, this AI is guided only by logic, self-consistency, and verifiable truth.&lt;/p&gt;
&lt;p&gt;What happens when AI no longer needs humans to improve?&lt;/p&gt;
&lt;p&gt;How DeepSeek-R1 Validates Its Own Reasoning&lt;/p&gt;
&lt;p&gt;DeepSeek-R1 doesn‚Äôt rely on human rankings or LLM-based reward models (like OpenAI‚Äôs RLHF approach). Instead, it uses rule-based evaluations, self-consistency, and majority voting to filter out incorrect or illogical reasoning.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Concept Convergence (Majority Voting)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The model generates multiple responses to the same question.&lt;/p&gt;
&lt;p&gt;It checks whether different responses agree on the same core concept using:&lt;/p&gt;
&lt;p&gt;Jaccard Similarity (word overlap)&lt;/p&gt;
&lt;p&gt;TF-IDF &amp;#x26; Cosine Similarity (text vector comparison)&lt;/p&gt;
&lt;p&gt;Sentence Embeddings (numerical meaning representation)&lt;/p&gt;
&lt;p&gt;Responses that agree with each other get rewarded. Outliers are penalized.&lt;/p&gt;
&lt;p&gt;Q: Why is the sky blue?A1: Rayleigh scattering causes blue light to scatter.A2: The atmosphere scatters short-wavelength blue light.A3: Sunlight interacts with air molecules, making blue light more visible.‚úÖ All responses point to Rayleigh scattering ‚Üí High reward‚ùå Diverging responses are filtered out.&lt;/p&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;Logical Consistency Checking&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If a response contradicts itself, it gets penalized.&lt;/p&gt;
&lt;p&gt;If the reasoning steps and final answer don‚Äôt match, it‚Äôs flagged.&lt;/p&gt;
&lt;p&gt; If A is greater than B, then B is greater than A.  B is greater than A. ‚ùå Breaks transitive logic ‚Üí Penalized.&lt;/p&gt;
&lt;ol start=&quot;3&quot;&gt;
&lt;li&gt;Formatting &amp;#x26; Readability&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Responses must follow the correct structure in both reasoning and final answer:&lt;/p&gt;
&lt;p&gt;Language consistency rules prevent mixed-language responses.&lt;/p&gt;
&lt;p&gt;Markdown formatting ensures clarity.&lt;/p&gt;
&lt;ol start=&quot;4&quot;&gt;
&lt;li&gt;Detecting Circular Reasoning &amp;#x26; Fallacies&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If the response just repeats the question, it gets penalized.&lt;/p&gt;
&lt;p&gt;If reasoning loops back on itself, dependency parsing detects it.&lt;/p&gt;
&lt;p&gt;Q: Why is the sky blue?A: Because the sky is blue in color.‚ùå Fails test ‚Üí Penalized.&lt;/p&gt;
&lt;ol start=&quot;5&quot;&gt;
&lt;li&gt;Factuality &amp;#x26; Safety Filtering&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Basic fact-checking against external sources like Wikipedia or scientific papers.&lt;/p&gt;
&lt;p&gt;Regex-based safety filters prevent harmful content.&lt;/p&gt;
&lt;p&gt;Q: Who discovered gravity?A: Albert Einstein.‚ùå Wrong ‚Üí Penalized.&lt;/p&gt;
&lt;p&gt;The Future of AI: Self-Training at Scale&lt;/p&gt;
&lt;p&gt;DeepSeek-R1 never needed humans to validate its reasoning. Instead, it relied on:
‚úî Semantic similarity &amp;#x26; majority voting to detect concept convergence.‚úî Logical consistency checks to flag contradictions.‚úî Regex-based format validation to enforce structure &amp;#x26; readability.‚úî Rule-based contradiction &amp;#x26; self-reference detection to catch bad logic.‚úî Fact-checking &amp;#x26; safety filters to ensure correctness &amp;#x26; alignment.&lt;/p&gt;
&lt;p&gt;This approach massively reduces human oversight costs and proves that LLMs can self-improve using pure reinforcement learning.&lt;/p&gt;
&lt;p&gt;Why This Could Be the Closest Step Toward AGI&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It Removes Human Bias from AI Training&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Traditional AI training relies on human opinions, which can be subjective and inconsistent.&lt;/p&gt;
&lt;p&gt;DeepSeek-R1 follows its own logic, aligning with truth rather than human expectations.&lt;/p&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;It Enables AI to Self-Correct&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Iterative reinforcement learning lets AI refine its own reasoning, discover new strategies, and correct flaws without human intervention.&lt;/p&gt;
&lt;p&gt;Instead of just mimicking humans, the AI develops its own logical framework, making it more autonomous.&lt;/p&gt;
&lt;ol start=&quot;3&quot;&gt;
&lt;li&gt;Large AI Companies Will Scale This With Their Massive Data Pools&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;OpenAI, DeepMind, and Meta have already trained multi-trillion-token models with huge compute budgets.&lt;/p&gt;
&lt;p&gt;They will likely apply reinforcement learning at scale, refining AI without human raters.&lt;/p&gt;
&lt;p&gt;üîπ What‚Äôs stopping them? Computation costs and avoiding reward hacking, but DeepSeek-R1‚Äôs rule-based approach shows these issues can be managed.&lt;/p&gt;
&lt;p&gt;The Next AI Gold Rush: Consolidating Knowledge at Scale&lt;/p&gt;
&lt;p&gt;If no human in the loop is required, then structured knowledge becomes the most valuable AI training resource.&lt;/p&gt;
&lt;p&gt;üîπ Why? AI needs clean, factual, structured knowledge to validate reasoning. The highest-value assets will be:&lt;/p&gt;
&lt;p&gt;Wikipedia &amp;#x26; curated knowledge bases&lt;/p&gt;
&lt;p&gt;Textbooks &amp;#x26; structured educational content&lt;/p&gt;
&lt;p&gt;Scientific journal articles &amp;#x26; research papers&lt;/p&gt;
&lt;p&gt;Legal &amp;#x26; historical records&lt;/p&gt;
&lt;p&gt;Medical datasets &amp;#x26; verified health guidelines&lt;/p&gt;
&lt;p&gt;Currently, these sources exist but are fragmented across different platforms. No AI system fully owns or centralizes them.&lt;/p&gt;
&lt;p&gt;The real opportunity?&lt;/p&gt;
&lt;p&gt;üìå A self-improving AI that can clean, merge, and structure all this knowledge autonomously.&lt;/p&gt;
&lt;p&gt;What Comes Next?&lt;/p&gt;
&lt;p&gt;If this strategy scales, we could see:‚úÖ Massive AI-driven knowledge repositories that consolidate global human knowledge.‚úÖ AI that continually refines its own understanding, rather than relying on human training.‚úÖ The end of the need for human alignment raters, replaced by logic-driven self-improvement.&lt;/p&gt;
&lt;p&gt;This is way beyond search engines‚Äîit‚Äôs AI actively reasoning over the world‚Äôs knowledge to create a coherent, evolving framework of truth.&lt;/p&gt;
&lt;p&gt;The Big Question: Is This a Good Thing?&lt;/p&gt;
&lt;p&gt;Will AI surpass human intelligence in reasoning-based tasks?&lt;/p&gt;
&lt;p&gt;What happens when AI can structure knowledge better than humans?&lt;/p&gt;
&lt;p&gt;Will centralized knowledge repositories be controlled or open-source?&lt;/p&gt;
&lt;p&gt;Could AI create its own scientific breakthroughs by consolidating and testing knowledge autonomously?&lt;/p&gt;
&lt;p&gt;This feels like the next frontier of AI. Are we ready for it?&lt;/p&gt;</content:encoded></item><item><title><![CDATA[The AI That Trains Itself: How DeepSeek-R1 Could Change Everything]]></title><description><![CDATA[For years, AI training has relied on human-guided alignment, where human raters score responses, and models are fine-tuned based on their‚Ä¶]]></description><link>https://www.spenserfiller.com/second-post/</link><guid isPermaLink="false">https://www.spenserfiller.com/second-post/</guid><pubDate>Wed, 29 Jan 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;For years, AI training has relied on &lt;strong&gt;human-guided alignment&lt;/strong&gt;, where human raters score responses, and models are fine-tuned based on their preferences. But DeepSeek-R1 has introduced something radically different‚Äîan AI that &lt;strong&gt;teaches itself&lt;/strong&gt;, refining its reasoning &lt;strong&gt;without human oversight or another AI acting as a judge&lt;/strong&gt;.  &lt;/p&gt;
&lt;p&gt;This isn‚Äôt just an optimization; it‚Äôs a &lt;strong&gt;fundamental shift in AI learning&lt;/strong&gt;. Instead of following human opinions, &lt;strong&gt;this AI is guided only by logic, self-consistency, and verifiable truth&lt;/strong&gt;.  &lt;/p&gt;
&lt;p&gt;What happens when AI no longer needs humans to improve?  &lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;strong&gt;How DeepSeek-R1 Validates Its Own Reasoning&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;DeepSeek-R1 doesn‚Äôt rely on &lt;strong&gt;human rankings&lt;/strong&gt; or &lt;strong&gt;LLM-based reward models&lt;/strong&gt; (like OpenAI‚Äôs RLHF-Reinforcement Learning from Human Feedback approach). Instead, it uses &lt;strong&gt;rule-based evaluations, self-consistency, and majority voting&lt;/strong&gt; to filter out incorrect or illogical reasoning.  &lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;1. Concept Convergence (Majority Voting)&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The model generates &lt;strong&gt;multiple responses&lt;/strong&gt; to the same question.  &lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It checks whether different responses &lt;strong&gt;agree on the same core concept&lt;/strong&gt; using:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Jaccard Similarity&lt;/strong&gt; (word overlap)  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TF-IDF &amp;#x26; Cosine Similarity&lt;/strong&gt; (text vector comparison)  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sentence Embeddings&lt;/strong&gt; (numerical meaning representation)  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Responses that &lt;strong&gt;agree with each other&lt;/strong&gt; get rewarded. Outliers are penalized.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;
&lt;strong&gt;Q:&lt;/strong&gt; Why is the sky blue?&lt;br&gt;
&lt;strong&gt;A1:&lt;/strong&gt; Rayleigh scattering causes blue light to scatter.&lt;br&gt;
&lt;strong&gt;A2:&lt;/strong&gt; The atmosphere scatters short-wavelength blue light.&lt;br&gt;
&lt;strong&gt;A3:&lt;/strong&gt; Sunlight interacts with air molecules, making blue light more visible.&lt;br&gt;
‚úÖ All responses point to Rayleigh scattering ‚Üí High reward&lt;br&gt;
‚ùå Diverging responses are filtered out.  &lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;2. Logical Consistency Checking&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;If a response contradicts itself, it gets penalized.  &lt;/li&gt;
&lt;li&gt;If the reasoning steps and final answer don‚Äôt match, it‚Äôs flagged.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example (Bad Logic):&lt;/strong&gt;
&lt;strong&gt;&lt;think&gt;&lt;/strong&gt; If A is greater than B, then B is greater than A. &lt;strong&gt;&lt;/think&gt;&lt;/strong&gt;&lt;br&gt;
&lt;strong&gt;&lt;answer&gt;&lt;/strong&gt; B is greater than A. &lt;strong&gt;&lt;/answer&gt;&lt;/strong&gt;&lt;br&gt;
‚ùå Breaks transitive logic ‚Üí Penalized.  &lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;3. Formatting &amp;#x26; Readability&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Responses must follow the correct structure:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;think&gt; reasoning &lt;/think&gt; &lt;answer&gt; final answer &lt;/answer&gt;&lt;/strong&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Language consistency rules prevent mixed-language responses.  &lt;/li&gt;
&lt;li&gt;Markdown formatting ensures clarity.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;strong&gt;4. Detecting Circular Reasoning &amp;#x26; Fallacies&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;If the response just repeats the question, it gets penalized.  &lt;/li&gt;
&lt;li&gt;If reasoning loops back on itself, dependency parsing detects it.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example (Circular Reasoning):&lt;/strong&gt;
&lt;strong&gt;Q:&lt;/strong&gt; Why is the sky blue?&lt;br&gt;
&lt;strong&gt;A:&lt;/strong&gt; Because the sky is blue in color.&lt;br&gt;
‚ùå Fails test ‚Üí Penalized.  &lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;5. Factuality &amp;#x26; Safety Filtering&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Basic fact-checking against external sources like Wikipedia or scientific papers.  &lt;/li&gt;
&lt;li&gt;Regex-based safety filters prevent harmful content.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;
&lt;strong&gt;Q:&lt;/strong&gt; Who discovered gravity?&lt;br&gt;
&lt;strong&gt;A:&lt;/strong&gt; Albert Einstein.&lt;br&gt;
‚ùå Wrong ‚Üí Penalized.  &lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;strong&gt;The Future of AI: Self-Training at Scale&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;DeepSeek-R1 never needed humans to validate its reasoning. Instead, it relied on:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Semantic similarity &amp;#x26; majority voting to detect concept convergence.  &lt;/li&gt;
&lt;li&gt;Logical consistency checks to flag contradictions.  &lt;/li&gt;
&lt;li&gt;Regex-based format validation to enforce structure &amp;#x26; readability.  &lt;/li&gt;
&lt;li&gt;Rule-based contradiction &amp;#x26; self-reference detection to catch bad logic.  &lt;/li&gt;
&lt;li&gt;Fact-checking &amp;#x26; safety filters to ensure correctness &amp;#x26; alignment.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This approach massively reduces human oversight costs and proves that LLMs can self-improve using pure reinforcement learning.  &lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;strong&gt;Why This Could Be the Closest Step Toward AGI&lt;/strong&gt;&lt;/h2&gt;
&lt;h3&gt;&lt;strong&gt;1. It Removes Human Bias from AI Training&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Traditional AI training relies on human opinions, which can be subjective and inconsistent.  &lt;/li&gt;
&lt;li&gt;DeepSeek-R1 follows its own logic, aligning with truth rather than human expectations.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;strong&gt;2. It Enables AI to Self-Correct&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Iterative reinforcement learning lets AI refine its own reasoning, discover new strategies, and correct flaws without human intervention.  &lt;/li&gt;
&lt;li&gt;Instead of just mimicking humans, the AI develops its own logical framework, making it more autonomous.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;strong&gt;3. Large AI Companies Will Scale This With Their Massive Data Pools&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;OpenAI, DeepMind, and Meta have already trained multi-trillion-token models with huge compute budgets.  &lt;/li&gt;
&lt;li&gt;They will likely apply reinforcement learning at scale, refining AI without human raters.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;What‚Äôs stopping them?&lt;/strong&gt; Computation costs and avoiding reward hacking, but DeepSeek-R1‚Äôs rule-based approach shows these issues can be managed.  &lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;strong&gt;The Next AI Gold Rush: Consolidating Knowledge at Scale&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;If no human in the loop is required, then structured knowledge becomes the most valuable AI training resource.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why?&lt;/strong&gt; AI needs clean, factual, structured knowledge to validate reasoning. The highest-value assets will be:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wikipedia &amp;#x26; curated knowledge bases  &lt;/li&gt;
&lt;li&gt;Textbooks &amp;#x26; structured educational content  &lt;/li&gt;
&lt;li&gt;Scientific journal articles &amp;#x26; research papers  &lt;/li&gt;
&lt;li&gt;Legal &amp;#x26; historical records  &lt;/li&gt;
&lt;li&gt;Medical datasets &amp;#x26; verified health guidelines  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Currently, these sources exist but are fragmented across different platforms. No AI system fully owns or centralizes them.  &lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;The real opportunity?&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;A self-improving AI that can clean, merge, and structure all this knowledge autonomously.  &lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;strong&gt;What Comes Next?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;If this strategy scales, we could see:&lt;br&gt;
Massive AI-driven knowledge repositories that consolidate global human knowledge.&lt;br&gt;
AI that continually refines its own understanding, rather than relying on human training.&lt;br&gt;
The end of the need for human alignment raters, replaced by logic-driven self-improvement.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;This is way beyond search engines‚Äîit‚Äôs AI actively reasoning over the world‚Äôs knowledge to create a coherent, evolving framework of truth.&lt;/strong&gt;  &lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;strong&gt;The Big Question: Is This a Good Thing?&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Will AI surpass human intelligence in reasoning-based tasks?  &lt;/li&gt;
&lt;li&gt;What happens when AI can structure knowledge better than humans?  &lt;/li&gt;
&lt;li&gt;Will centralized knowledge repositories be controlled or open-source?  &lt;/li&gt;
&lt;li&gt;Could AI create its own scientific breakthroughs by consolidating and testing knowledge autonomously?  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;This feels like the next frontier of AI‚Äîare we ready for it?&lt;/strong&gt;  &lt;/p&gt;</content:encoded></item><item><title><![CDATA[Introduction]]></title><description><![CDATA[This is gonna be the start to some thoughts. I‚Äôm not really sure where I want to go with it, but I‚Äôd like a place to be creative and host‚Ä¶]]></description><link>https://www.spenserfiller.com/first-post/</link><guid isPermaLink="false">https://www.spenserfiller.com/first-post/</guid><pubDate>Sun, 03 May 2020 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;This is gonna be the start to some thoughts. I‚Äôm not really sure where I want to go with it, but I‚Äôd like a place to be creative and host things of my own.&lt;/p&gt;
&lt;p&gt;It‚Äôs a notebook of sorts. I may keep track of workouts, recipes, and other thoughts. It could be used to create a framework or set of principles for my life.&lt;/p&gt;
&lt;p&gt;This is primarily here for me, but sharing is caring.&lt;/p&gt;</content:encoded></item></channel></rss>