<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Notebook]]></title><description><![CDATA[A place for thoughts and stuff]]></description><link>https://www.spenserfiller.com</link><generator>GatsbyJS</generator><lastBuildDate>Fri, 31 Jan 2025 23:55:47 GMT</lastBuildDate><item><title><![CDATA[Why Do Neural Networks Need Activation Functions?]]></title><description><![CDATA[Understanding Why Depth Needs Non-Linearity Deep learning models are built from layers of neurons, but one of the most important ingredients…]]></description><link>https://www.spenserfiller.com/third-post/</link><guid isPermaLink="false">https://www.spenserfiller.com/third-post/</guid><pubDate>Fri, 31 Jan 2025 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;Understanding Why Depth Needs Non-Linearity&lt;/h2&gt;
&lt;p&gt;Deep learning models are built from layers of neurons, but one of the most important ingredients that makes them work effectively is the activation function.&lt;/p&gt;
&lt;p&gt;If you’ve ever wondered:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Why can’t we just stack many linear layers?&lt;/li&gt;
&lt;li&gt;What does an activation function actually do?&lt;/li&gt;
&lt;li&gt;How does setting some values to 0 help learning?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This post will break it all down in an intuitive way.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;1. What Happens Without Activation Functions?&lt;/h2&gt;
&lt;p&gt;At their core, neural networks perform a series of mathematical transformations on input data. A simple linear layer applies the transformation:&lt;/p&gt;
&lt;p&gt;[ y = Wx + b ]&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;( W ) is a weight matrix that scales inputs,&lt;/li&gt;
&lt;li&gt;( x ) is the input vector (e.g., an image, text embedding, etc.),&lt;/li&gt;
&lt;li&gt;( b ) is a bias term that shifts the output.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we stack multiple linear layers, you might expect the network to learn more complex representations. However, something surprising happens:&lt;/p&gt;
&lt;p&gt;[ h&lt;em&gt;1 = W&lt;/em&gt;1 x + b&lt;em&gt;1 ]
[ h&lt;/em&gt;2 = W&lt;em&gt;2 h&lt;/em&gt;1 + b_2 ]&lt;/p&gt;
&lt;p&gt;Substituting ( h_1 ):&lt;/p&gt;
&lt;p&gt;[ h&lt;em&gt;2 = W&lt;/em&gt;2 (W&lt;em&gt;1 x + b&lt;/em&gt;1) + b&lt;em&gt;2 ]
[ h&lt;/em&gt;2 = (W&lt;em&gt;2 W&lt;/em&gt;1) x + (W&lt;em&gt;2 b&lt;/em&gt;1 + b_2) ]&lt;/p&gt;
&lt;p&gt;This is still just one big linear function! No matter how many layers we stack, we never get the ability to model complex, non-linear patterns.&lt;/p&gt;
&lt;p&gt;This means:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The network can only separate data using straight lines (or planes in higher dimensions).&lt;/li&gt;
&lt;li&gt;It cannot adapt to curved decision boundaries needed for real-world problems.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;2. How Activation Functions Break This Limitation&lt;/h2&gt;
&lt;p&gt;To make neural networks truly powerful, we need to introduce non-linearity. This is where activation functions come in.&lt;/p&gt;
&lt;p&gt;When we apply an activation function ( f(x) ) after each layer:&lt;/p&gt;
&lt;p&gt;[ h&lt;em&gt;1 = f(W&lt;/em&gt;1 x + b&lt;em&gt;1) ]
[ h&lt;/em&gt;2 = f(W&lt;em&gt;2 h&lt;/em&gt;1 + b_2) ]&lt;/p&gt;
&lt;p&gt;Now, we can’t simplify the layers into one big equation—each layer transforms the data differently based on the activation function, allowing the network to model complex relationships.&lt;/p&gt;
&lt;p&gt;But why does this work?&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;3. How Does an Activation Function “Break It Up”?&lt;/h2&gt;
&lt;p&gt;The key to activation functions is that they change how information flows between layers.&lt;/p&gt;
&lt;p&gt;Without activation functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each layer predictably transforms the input in a fixed, linear way.&lt;/li&gt;
&lt;li&gt;The next layer can always adjust for the previous layer’s transformation, meaning the network behaves like one big layer.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With activation functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Certain values get modified (or set to zero).&lt;/li&gt;
&lt;li&gt;This forces the next layer to adapt in different ways depending on the input.&lt;/li&gt;
&lt;li&gt;Instead of a single, predictable transformation, the network now learns piecewise, non-linear functions that approximate curves.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example: ReLU Activation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;[ \text{ReLU}(x) = \max(0, x) ]&lt;/p&gt;
&lt;p&gt;If ( x ) is negative, it becomes 0 (neuron turns off). If ( x ) is positive, it stays the same (neuron passes information forward).&lt;/p&gt;
&lt;p&gt;This breaks linearity because some neurons completely stop contributing in certain areas of the input space. The next layer must now adapt based on which neurons are active, rather than always seeing the same structured transformation.&lt;/p&gt;
&lt;p&gt;This is what lets deep networks approximate curves instead of just drawing straight lines.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;4. Why ReLU or GELU?&lt;/h2&gt;
&lt;p&gt;ReLU (Rectified Linear Unit) is the most common activation function because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It’s simple and fast (just a max operation).&lt;/li&gt;
&lt;li&gt;It prevents vanishing gradients (unlike Sigmoid and Tanh).&lt;/li&gt;
&lt;li&gt;It creates sparse activations, helping the model learn more efficiently.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, ReLU has a problem: some neurons “die” and never activate (if they always receive negative inputs).&lt;/p&gt;
&lt;p&gt;GELU (Gaussian Error Linear Unit) is a smoother alternative used in models like Transformers (GPT, BERT). It behaves like ReLU for large values but gradually scales small negative values instead of cutting them off completely. This leads to better training stability in deep models.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use ReLU for most applications (CNNs, general deep learning).&lt;/li&gt;
&lt;li&gt;Use GELU for deep architectures where smooth activation helps, like NLP models.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;5. Final Thoughts: Activation Functions Are the Key to Depth&lt;/h2&gt;
&lt;p&gt;If there’s one thing to take away from this post, it’s this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Without activation functions, neural networks are just big linear equations.&lt;/li&gt;
&lt;li&gt;Activation functions “break up” predictability, enabling deep networks to learn complex patterns.&lt;/li&gt;
&lt;li&gt;ReLU is simple and fast; GELU is smoother and better for deep architectures.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Deep learning isn’t just about stacking layers—it’s about stacking them in a way that introduces the right kinds of transformations. Activation functions are the secret sauce that make deep learning truly deep.&lt;/p&gt;
&lt;hr&gt;
&lt;h3&gt;Want to Go Deeper?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Try experimenting with activation functions in PyTorch or TensorFlow.&lt;/li&gt;
&lt;li&gt;Look at how ReLU and GELU behave visually.&lt;/li&gt;
&lt;li&gt;Read more about how activation functions impact learning dynamics.&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[The AI That Trains Itself: How DeepSeek-R1 Could Change Everything]]></title><description><![CDATA[For years, AI training has relied on human-guided alignment, where human raters score responses, and models are fine-tuned based on their…]]></description><link>https://www.spenserfiller.com/second-post/</link><guid isPermaLink="false">https://www.spenserfiller.com/second-post/</guid><pubDate>Wed, 29 Jan 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;For years, AI training has relied on &lt;strong&gt;human-guided alignment&lt;/strong&gt;, where human raters score responses, and models are fine-tuned based on their preferences. But DeepSeek-R1 has introduced something radically different—an AI that &lt;strong&gt;teaches itself&lt;/strong&gt;, refining its reasoning &lt;strong&gt;without human oversight or another AI acting as a judge&lt;/strong&gt;.  &lt;/p&gt;
&lt;p&gt;This isn’t just an optimization; it’s a &lt;strong&gt;fundamental shift in AI learning&lt;/strong&gt;. Instead of following human opinions, &lt;strong&gt;this AI is guided only by logic, self-consistency, and verifiable truth&lt;/strong&gt;.  &lt;/p&gt;
&lt;p&gt;What happens when AI no longer needs humans to improve?  &lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;strong&gt;How DeepSeek-R1 Validates Its Own Reasoning&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;DeepSeek-R1 doesn’t rely on &lt;strong&gt;human rankings&lt;/strong&gt; or &lt;strong&gt;LLM-based reward models&lt;/strong&gt; (like OpenAI’s RLHF-Reinforcement Learning from Human Feedback approach). Instead, it uses &lt;strong&gt;rule-based evaluations, self-consistency, and majority voting&lt;/strong&gt; to filter out incorrect or illogical reasoning.  &lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;1. Concept Convergence (Majority Voting)&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The model generates &lt;strong&gt;multiple responses&lt;/strong&gt; to the same question.  &lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It checks whether different responses &lt;strong&gt;agree on the same core concept&lt;/strong&gt; using:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Jaccard Similarity&lt;/strong&gt; (word overlap)  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TF-IDF &amp;#x26; Cosine Similarity&lt;/strong&gt; (text vector comparison)  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sentence Embeddings&lt;/strong&gt; (numerical meaning representation)  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Responses that &lt;strong&gt;agree with each other&lt;/strong&gt; get rewarded. Outliers are penalized.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;
&lt;strong&gt;Q:&lt;/strong&gt; Why is the sky blue?&lt;br&gt;
&lt;strong&gt;A1:&lt;/strong&gt; Rayleigh scattering causes blue light to scatter.&lt;br&gt;
&lt;strong&gt;A2:&lt;/strong&gt; The atmosphere scatters short-wavelength blue light.&lt;br&gt;
&lt;strong&gt;A3:&lt;/strong&gt; Sunlight interacts with air molecules, making blue light more visible.&lt;br&gt;
✅ All responses point to Rayleigh scattering → High reward&lt;br&gt;
❌ Diverging responses are filtered out.  &lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;2. Logical Consistency Checking&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;If a response contradicts itself, it gets penalized.  &lt;/li&gt;
&lt;li&gt;If the reasoning steps and final answer don’t match, it’s flagged.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example (Bad Logic):&lt;/strong&gt;
&lt;strong&gt;&lt;think&gt;&lt;/strong&gt; If A is greater than B, then B is greater than A. &lt;strong&gt;&lt;/think&gt;&lt;/strong&gt;&lt;br&gt;
&lt;strong&gt;&lt;answer&gt;&lt;/strong&gt; B is greater than A. &lt;strong&gt;&lt;/answer&gt;&lt;/strong&gt;&lt;br&gt;
❌ Breaks transitive logic → Penalized.  &lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;3. Formatting &amp;#x26; Readability&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Responses must follow the correct structure:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;think&gt; reasoning &lt;/think&gt; &lt;answer&gt; final answer &lt;/answer&gt;&lt;/strong&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Language consistency rules prevent mixed-language responses.  &lt;/li&gt;
&lt;li&gt;Markdown formatting ensures clarity.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;strong&gt;4. Detecting Circular Reasoning &amp;#x26; Fallacies&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;If the response just repeats the question, it gets penalized.  &lt;/li&gt;
&lt;li&gt;If reasoning loops back on itself, dependency parsing detects it.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example (Circular Reasoning):&lt;/strong&gt;
&lt;strong&gt;Q:&lt;/strong&gt; Why is the sky blue?&lt;br&gt;
&lt;strong&gt;A:&lt;/strong&gt; Because the sky is blue in color.&lt;br&gt;
❌ Fails test → Penalized.  &lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;5. Factuality &amp;#x26; Safety Filtering&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Basic fact-checking against external sources like Wikipedia or scientific papers.  &lt;/li&gt;
&lt;li&gt;Regex-based safety filters prevent harmful content.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;
&lt;strong&gt;Q:&lt;/strong&gt; Who discovered gravity?&lt;br&gt;
&lt;strong&gt;A:&lt;/strong&gt; Albert Einstein.&lt;br&gt;
❌ Wrong → Penalized.  &lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;strong&gt;The Future of AI: Self-Training at Scale&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;DeepSeek-R1 never needed humans to validate its reasoning. Instead, it relied on:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Semantic similarity &amp;#x26; majority voting to detect concept convergence.  &lt;/li&gt;
&lt;li&gt;Logical consistency checks to flag contradictions.  &lt;/li&gt;
&lt;li&gt;Regex-based format validation to enforce structure &amp;#x26; readability.  &lt;/li&gt;
&lt;li&gt;Rule-based contradiction &amp;#x26; self-reference detection to catch bad logic.  &lt;/li&gt;
&lt;li&gt;Fact-checking &amp;#x26; safety filters to ensure correctness &amp;#x26; alignment.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This approach massively reduces human oversight costs and proves that LLMs can self-improve using pure reinforcement learning.  &lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;strong&gt;Why This Could Be the Closest Step Toward AGI&lt;/strong&gt;&lt;/h2&gt;
&lt;h3&gt;&lt;strong&gt;1. It Removes Human Bias from AI Training&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Traditional AI training relies on human opinions, which can be subjective and inconsistent.  &lt;/li&gt;
&lt;li&gt;DeepSeek-R1 follows its own logic, aligning with truth rather than human expectations.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;strong&gt;2. It Enables AI to Self-Correct&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Iterative reinforcement learning lets AI refine its own reasoning, discover new strategies, and correct flaws without human intervention.  &lt;/li&gt;
&lt;li&gt;Instead of just mimicking humans, the AI develops its own logical framework, making it more autonomous.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;strong&gt;3. Large AI Companies Will Scale This With Their Massive Data Pools&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;OpenAI, DeepMind, and Meta have already trained multi-trillion-token models with huge compute budgets.  &lt;/li&gt;
&lt;li&gt;They will likely apply reinforcement learning at scale, refining AI without human raters.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;What’s stopping them?&lt;/strong&gt; Computation costs and avoiding reward hacking, but DeepSeek-R1’s rule-based approach shows these issues can be managed.  &lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;strong&gt;The Next AI Gold Rush: Consolidating Knowledge at Scale&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;If no human in the loop is required, then structured knowledge becomes the most valuable AI training resource.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why?&lt;/strong&gt; AI needs clean, factual, structured knowledge to validate reasoning. The highest-value assets will be:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wikipedia &amp;#x26; curated knowledge bases  &lt;/li&gt;
&lt;li&gt;Textbooks &amp;#x26; structured educational content  &lt;/li&gt;
&lt;li&gt;Scientific journal articles &amp;#x26; research papers  &lt;/li&gt;
&lt;li&gt;Legal &amp;#x26; historical records  &lt;/li&gt;
&lt;li&gt;Medical datasets &amp;#x26; verified health guidelines  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Currently, these sources exist but are fragmented across different platforms. No AI system fully owns or centralizes them.  &lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;The real opportunity?&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;A self-improving AI that can clean, merge, and structure all this knowledge autonomously.  &lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;strong&gt;What Comes Next?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;If this strategy scales, we could see:&lt;br&gt;
Massive AI-driven knowledge repositories that consolidate global human knowledge.&lt;br&gt;
AI that continually refines its own understanding, rather than relying on human training.&lt;br&gt;
The end of the need for human alignment raters, replaced by logic-driven self-improvement.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;This is way beyond search engines—it’s AI actively reasoning over the world’s knowledge to create a coherent, evolving framework of truth.&lt;/strong&gt;  &lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;strong&gt;The Big Question: Is This a Good Thing?&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Will AI surpass human intelligence in reasoning-based tasks?  &lt;/li&gt;
&lt;li&gt;What happens when AI can structure knowledge better than humans?  &lt;/li&gt;
&lt;li&gt;Will centralized knowledge repositories be controlled or open-source?  &lt;/li&gt;
&lt;li&gt;Could AI create its own scientific breakthroughs by consolidating and testing knowledge autonomously?  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;This feels like the next frontier of AI—are we ready for it?&lt;/strong&gt;  &lt;/p&gt;</content:encoded></item><item><title><![CDATA[The AI That Trains Itself: How DeepSeek-R1 Could Change Everything]]></title><description><![CDATA[For years, AI training has relied on human-guided alignment, where human raters score responses, and models are fine-tuned based on their…]]></description><link>https://www.spenserfiller.com/second-post/deepseek-r1-blog/</link><guid isPermaLink="false">https://www.spenserfiller.com/second-post/deepseek-r1-blog/</guid><pubDate>Wed, 29 Jan 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;For years, AI training has relied on human-guided alignment, where human raters score responses, and models are fine-tuned based on their preferences. But DeepSeek-R1 has introduced something radically different—an AI that teaches itself, refining its reasoning without human oversight or another AI acting as a judge.&lt;/p&gt;
&lt;p&gt;This isn’t just an optimization; it’s a fundamental shift in AI learning. Instead of following human opinions, this AI is guided only by logic, self-consistency, and verifiable truth.&lt;/p&gt;
&lt;p&gt;What happens when AI no longer needs humans to improve?&lt;/p&gt;
&lt;p&gt;How DeepSeek-R1 Validates Its Own Reasoning&lt;/p&gt;
&lt;p&gt;DeepSeek-R1 doesn’t rely on human rankings or LLM-based reward models (like OpenAI’s RLHF approach). Instead, it uses rule-based evaluations, self-consistency, and majority voting to filter out incorrect or illogical reasoning.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Concept Convergence (Majority Voting)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The model generates multiple responses to the same question.&lt;/p&gt;
&lt;p&gt;It checks whether different responses agree on the same core concept using:&lt;/p&gt;
&lt;p&gt;Jaccard Similarity (word overlap)&lt;/p&gt;
&lt;p&gt;TF-IDF &amp;#x26; Cosine Similarity (text vector comparison)&lt;/p&gt;
&lt;p&gt;Sentence Embeddings (numerical meaning representation)&lt;/p&gt;
&lt;p&gt;Responses that agree with each other get rewarded. Outliers are penalized.&lt;/p&gt;
&lt;p&gt;Q: Why is the sky blue?A1: Rayleigh scattering causes blue light to scatter.A2: The atmosphere scatters short-wavelength blue light.A3: Sunlight interacts with air molecules, making blue light more visible.✅ All responses point to Rayleigh scattering → High reward❌ Diverging responses are filtered out.&lt;/p&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;Logical Consistency Checking&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If a response contradicts itself, it gets penalized.&lt;/p&gt;
&lt;p&gt;If the reasoning steps and final answer don’t match, it’s flagged.&lt;/p&gt;
&lt;p&gt; If A is greater than B, then B is greater than A.  B is greater than A. ❌ Breaks transitive logic → Penalized.&lt;/p&gt;
&lt;ol start=&quot;3&quot;&gt;
&lt;li&gt;Formatting &amp;#x26; Readability&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Responses must follow the correct structure in both reasoning and final answer:&lt;/p&gt;
&lt;p&gt;Language consistency rules prevent mixed-language responses.&lt;/p&gt;
&lt;p&gt;Markdown formatting ensures clarity.&lt;/p&gt;
&lt;ol start=&quot;4&quot;&gt;
&lt;li&gt;Detecting Circular Reasoning &amp;#x26; Fallacies&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If the response just repeats the question, it gets penalized.&lt;/p&gt;
&lt;p&gt;If reasoning loops back on itself, dependency parsing detects it.&lt;/p&gt;
&lt;p&gt;Q: Why is the sky blue?A: Because the sky is blue in color.❌ Fails test → Penalized.&lt;/p&gt;
&lt;ol start=&quot;5&quot;&gt;
&lt;li&gt;Factuality &amp;#x26; Safety Filtering&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Basic fact-checking against external sources like Wikipedia or scientific papers.&lt;/p&gt;
&lt;p&gt;Regex-based safety filters prevent harmful content.&lt;/p&gt;
&lt;p&gt;Q: Who discovered gravity?A: Albert Einstein.❌ Wrong → Penalized.&lt;/p&gt;
&lt;p&gt;The Future of AI: Self-Training at Scale&lt;/p&gt;
&lt;p&gt;DeepSeek-R1 never needed humans to validate its reasoning. Instead, it relied on:
✔ Semantic similarity &amp;#x26; majority voting to detect concept convergence.✔ Logical consistency checks to flag contradictions.✔ Regex-based format validation to enforce structure &amp;#x26; readability.✔ Rule-based contradiction &amp;#x26; self-reference detection to catch bad logic.✔ Fact-checking &amp;#x26; safety filters to ensure correctness &amp;#x26; alignment.&lt;/p&gt;
&lt;p&gt;This approach massively reduces human oversight costs and proves that LLMs can self-improve using pure reinforcement learning.&lt;/p&gt;
&lt;p&gt;Why This Could Be the Closest Step Toward AGI&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It Removes Human Bias from AI Training&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Traditional AI training relies on human opinions, which can be subjective and inconsistent.&lt;/p&gt;
&lt;p&gt;DeepSeek-R1 follows its own logic, aligning with truth rather than human expectations.&lt;/p&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;It Enables AI to Self-Correct&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Iterative reinforcement learning lets AI refine its own reasoning, discover new strategies, and correct flaws without human intervention.&lt;/p&gt;
&lt;p&gt;Instead of just mimicking humans, the AI develops its own logical framework, making it more autonomous.&lt;/p&gt;
&lt;ol start=&quot;3&quot;&gt;
&lt;li&gt;Large AI Companies Will Scale This With Their Massive Data Pools&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;OpenAI, DeepMind, and Meta have already trained multi-trillion-token models with huge compute budgets.&lt;/p&gt;
&lt;p&gt;They will likely apply reinforcement learning at scale, refining AI without human raters.&lt;/p&gt;
&lt;p&gt;🔹 What’s stopping them? Computation costs and avoiding reward hacking, but DeepSeek-R1’s rule-based approach shows these issues can be managed.&lt;/p&gt;
&lt;p&gt;The Next AI Gold Rush: Consolidating Knowledge at Scale&lt;/p&gt;
&lt;p&gt;If no human in the loop is required, then structured knowledge becomes the most valuable AI training resource.&lt;/p&gt;
&lt;p&gt;🔹 Why? AI needs clean, factual, structured knowledge to validate reasoning. The highest-value assets will be:&lt;/p&gt;
&lt;p&gt;Wikipedia &amp;#x26; curated knowledge bases&lt;/p&gt;
&lt;p&gt;Textbooks &amp;#x26; structured educational content&lt;/p&gt;
&lt;p&gt;Scientific journal articles &amp;#x26; research papers&lt;/p&gt;
&lt;p&gt;Legal &amp;#x26; historical records&lt;/p&gt;
&lt;p&gt;Medical datasets &amp;#x26; verified health guidelines&lt;/p&gt;
&lt;p&gt;Currently, these sources exist but are fragmented across different platforms. No AI system fully owns or centralizes them.&lt;/p&gt;
&lt;p&gt;The real opportunity?&lt;/p&gt;
&lt;p&gt;📌 A self-improving AI that can clean, merge, and structure all this knowledge autonomously.&lt;/p&gt;
&lt;p&gt;What Comes Next?&lt;/p&gt;
&lt;p&gt;If this strategy scales, we could see:✅ Massive AI-driven knowledge repositories that consolidate global human knowledge.✅ AI that continually refines its own understanding, rather than relying on human training.✅ The end of the need for human alignment raters, replaced by logic-driven self-improvement.&lt;/p&gt;
&lt;p&gt;This is way beyond search engines—it’s AI actively reasoning over the world’s knowledge to create a coherent, evolving framework of truth.&lt;/p&gt;
&lt;p&gt;The Big Question: Is This a Good Thing?&lt;/p&gt;
&lt;p&gt;Will AI surpass human intelligence in reasoning-based tasks?&lt;/p&gt;
&lt;p&gt;What happens when AI can structure knowledge better than humans?&lt;/p&gt;
&lt;p&gt;Will centralized knowledge repositories be controlled or open-source?&lt;/p&gt;
&lt;p&gt;Could AI create its own scientific breakthroughs by consolidating and testing knowledge autonomously?&lt;/p&gt;
&lt;p&gt;This feels like the next frontier of AI. Are we ready for it?&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Introduction]]></title><description><![CDATA[This is gonna be the start to some thoughts. I’m not really sure where I want to go with it, but I’d like a place to be creative and host…]]></description><link>https://www.spenserfiller.com/first-post/</link><guid isPermaLink="false">https://www.spenserfiller.com/first-post/</guid><pubDate>Sun, 03 May 2020 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;This is gonna be the start to some thoughts. I’m not really sure where I want to go with it, but I’d like a place to be creative and host things of my own.&lt;/p&gt;
&lt;p&gt;It’s a notebook of sorts. I may keep track of workouts, recipes, and other thoughts. It could be used to create a framework or set of principles for my life.&lt;/p&gt;
&lt;p&gt;This is primarily here for me, but sharing is caring.&lt;/p&gt;</content:encoded></item></channel></rss>