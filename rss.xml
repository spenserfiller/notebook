<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Notebook]]></title><description><![CDATA[A place for thoughts and stuff]]></description><link>https://www.spenserfiller.com</link><generator>GatsbyJS</generator><lastBuildDate>Fri, 31 Jan 2025 04:36:24 GMT</lastBuildDate><item><title><![CDATA[The AI That Trains Itself: How DeepSeek-R1 Could Change Everything]]></title><description><![CDATA[For years, AI training has relied on human-guided alignment, where human raters score responses, and models are fine-tuned based on their…]]></description><link>https://www.spenserfiller.com/second-post/</link><guid isPermaLink="false">https://www.spenserfiller.com/second-post/</guid><pubDate>Wed, 29 Jan 2025 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;For years, AI training has relied on &lt;strong&gt;human-guided alignment&lt;/strong&gt;, where human raters score responses, and models are fine-tuned based on their preferences. But DeepSeek-R1 has introduced something radically different—an AI that &lt;strong&gt;teaches itself&lt;/strong&gt;, refining its reasoning &lt;strong&gt;without human oversight or another AI acting as a judge&lt;/strong&gt;.  &lt;/p&gt;
&lt;p&gt;This isn’t just an optimization; it’s a &lt;strong&gt;fundamental shift in AI learning&lt;/strong&gt;. Instead of following human opinions, &lt;strong&gt;this AI is guided only by logic, self-consistency, and verifiable truth&lt;/strong&gt;.  &lt;/p&gt;
&lt;p&gt;What happens when AI no longer needs humans to improve?  &lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;strong&gt;How DeepSeek-R1 Validates Its Own Reasoning&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;DeepSeek-R1 doesn’t rely on &lt;strong&gt;human rankings&lt;/strong&gt; or &lt;strong&gt;LLM-based reward models&lt;/strong&gt; (like OpenAI’s RLHF-Reinforcement Learning from Human Feedback approach). Instead, it uses &lt;strong&gt;rule-based evaluations, self-consistency, and majority voting&lt;/strong&gt; to filter out incorrect or illogical reasoning.  &lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;1. Concept Convergence (Majority Voting)&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The model generates &lt;strong&gt;multiple responses&lt;/strong&gt; to the same question.  &lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It checks whether different responses &lt;strong&gt;agree on the same core concept&lt;/strong&gt; using:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Jaccard Similarity&lt;/strong&gt; (word overlap)  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TF-IDF &amp;#x26; Cosine Similarity&lt;/strong&gt; (text vector comparison)  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sentence Embeddings&lt;/strong&gt; (numerical meaning representation)  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Responses that &lt;strong&gt;agree with each other&lt;/strong&gt; get rewarded. Outliers are penalized.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;
&lt;strong&gt;Q:&lt;/strong&gt; Why is the sky blue?&lt;br&gt;
&lt;strong&gt;A1:&lt;/strong&gt; Rayleigh scattering causes blue light to scatter.&lt;br&gt;
&lt;strong&gt;A2:&lt;/strong&gt; The atmosphere scatters short-wavelength blue light.&lt;br&gt;
&lt;strong&gt;A3:&lt;/strong&gt; Sunlight interacts with air molecules, making blue light more visible.&lt;br&gt;
✅ All responses point to Rayleigh scattering → High reward&lt;br&gt;
❌ Diverging responses are filtered out.  &lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;2. Logical Consistency Checking&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;If a response contradicts itself, it gets penalized.  &lt;/li&gt;
&lt;li&gt;If the reasoning steps and final answer don’t match, it’s flagged.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example (Bad Logic):&lt;/strong&gt;
&lt;strong&gt;&lt;think&gt;&lt;/strong&gt; If A is greater than B, then B is greater than A. &lt;strong&gt;&lt;/think&gt;&lt;/strong&gt;&lt;br&gt;
&lt;strong&gt;&lt;answer&gt;&lt;/strong&gt; B is greater than A. &lt;strong&gt;&lt;/answer&gt;&lt;/strong&gt;&lt;br&gt;
❌ Breaks transitive logic → Penalized.  &lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;3. Formatting &amp;#x26; Readability&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Responses must follow the correct structure:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;think&gt; reasoning &lt;/think&gt; &lt;answer&gt; final answer &lt;/answer&gt;&lt;/strong&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Language consistency rules prevent mixed-language responses.  &lt;/li&gt;
&lt;li&gt;Markdown formatting ensures clarity.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;strong&gt;4. Detecting Circular Reasoning &amp;#x26; Fallacies&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;If the response just repeats the question, it gets penalized.  &lt;/li&gt;
&lt;li&gt;If reasoning loops back on itself, dependency parsing detects it.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example (Circular Reasoning):&lt;/strong&gt;
&lt;strong&gt;Q:&lt;/strong&gt; Why is the sky blue?&lt;br&gt;
&lt;strong&gt;A:&lt;/strong&gt; Because the sky is blue in color.&lt;br&gt;
❌ Fails test → Penalized.  &lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;5. Factuality &amp;#x26; Safety Filtering&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Basic fact-checking against external sources like Wikipedia or scientific papers.  &lt;/li&gt;
&lt;li&gt;Regex-based safety filters prevent harmful content.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;
&lt;strong&gt;Q:&lt;/strong&gt; Who discovered gravity?&lt;br&gt;
&lt;strong&gt;A:&lt;/strong&gt; Albert Einstein.&lt;br&gt;
❌ Wrong → Penalized.  &lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;strong&gt;The Future of AI: Self-Training at Scale&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;DeepSeek-R1 never needed humans to validate its reasoning. Instead, it relied on:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Semantic similarity &amp;#x26; majority voting to detect concept convergence.  &lt;/li&gt;
&lt;li&gt;Logical consistency checks to flag contradictions.  &lt;/li&gt;
&lt;li&gt;Regex-based format validation to enforce structure &amp;#x26; readability.  &lt;/li&gt;
&lt;li&gt;Rule-based contradiction &amp;#x26; self-reference detection to catch bad logic.  &lt;/li&gt;
&lt;li&gt;Fact-checking &amp;#x26; safety filters to ensure correctness &amp;#x26; alignment.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This approach massively reduces human oversight costs and proves that LLMs can self-improve using pure reinforcement learning.  &lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;strong&gt;Why This Could Be the Closest Step Toward AGI&lt;/strong&gt;&lt;/h2&gt;
&lt;h3&gt;&lt;strong&gt;1. It Removes Human Bias from AI Training&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Traditional AI training relies on human opinions, which can be subjective and inconsistent.  &lt;/li&gt;
&lt;li&gt;DeepSeek-R1 follows its own logic, aligning with truth rather than human expectations.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;strong&gt;2. It Enables AI to Self-Correct&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Iterative reinforcement learning lets AI refine its own reasoning, discover new strategies, and correct flaws without human intervention.  &lt;/li&gt;
&lt;li&gt;Instead of just mimicking humans, the AI develops its own logical framework, making it more autonomous.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;strong&gt;3. Large AI Companies Will Scale This With Their Massive Data Pools&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;OpenAI, DeepMind, and Meta have already trained multi-trillion-token models with huge compute budgets.  &lt;/li&gt;
&lt;li&gt;They will likely apply reinforcement learning at scale, refining AI without human raters.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;What’s stopping them?&lt;/strong&gt; Computation costs and avoiding reward hacking, but DeepSeek-R1’s rule-based approach shows these issues can be managed.  &lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;strong&gt;The Next AI Gold Rush: Consolidating Knowledge at Scale&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;If no human in the loop is required, then structured knowledge becomes the most valuable AI training resource.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why?&lt;/strong&gt; AI needs clean, factual, structured knowledge to validate reasoning. The highest-value assets will be:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wikipedia &amp;#x26; curated knowledge bases  &lt;/li&gt;
&lt;li&gt;Textbooks &amp;#x26; structured educational content  &lt;/li&gt;
&lt;li&gt;Scientific journal articles &amp;#x26; research papers  &lt;/li&gt;
&lt;li&gt;Legal &amp;#x26; historical records  &lt;/li&gt;
&lt;li&gt;Medical datasets &amp;#x26; verified health guidelines  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Currently, these sources exist but are fragmented across different platforms. No AI system fully owns or centralizes them.  &lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;The real opportunity?&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;A self-improving AI that can clean, merge, and structure all this knowledge autonomously.  &lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;strong&gt;What Comes Next?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;If this strategy scales, we could see:&lt;br&gt;
Massive AI-driven knowledge repositories that consolidate global human knowledge.&lt;br&gt;
AI that continually refines its own understanding, rather than relying on human training.&lt;br&gt;
The end of the need for human alignment raters, replaced by logic-driven self-improvement.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;This is way beyond search engines—it’s AI actively reasoning over the world’s knowledge to create a coherent, evolving framework of truth.&lt;/strong&gt;  &lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;strong&gt;The Big Question: Is This a Good Thing?&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Will AI surpass human intelligence in reasoning-based tasks?  &lt;/li&gt;
&lt;li&gt;What happens when AI can structure knowledge better than humans?  &lt;/li&gt;
&lt;li&gt;Will centralized knowledge repositories be controlled or open-source?  &lt;/li&gt;
&lt;li&gt;Could AI create its own scientific breakthroughs by consolidating and testing knowledge autonomously?  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;This feels like the next frontier of AI—are we ready for it?&lt;/strong&gt;  &lt;/p&gt;</content:encoded></item><item><title><![CDATA[Introduction]]></title><description><![CDATA[This is gonna be the start to some thoughts. I’m not really sure where I want to go with it, but I’d like a place to be creative and host…]]></description><link>https://www.spenserfiller.com/first-post/</link><guid isPermaLink="false">https://www.spenserfiller.com/first-post/</guid><pubDate>Sun, 03 May 2020 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;This is gonna be the start to some thoughts. I’m not really sure where I want to go with it, but I’d like a place to be creative and host things of my own.&lt;/p&gt;
&lt;p&gt;It’s a notebook of sorts. I may keep track of workouts, recipes, and other thoughts. It could be used to create a framework or set of principles for my life.&lt;/p&gt;
&lt;p&gt;This is primarily here for me, but sharing is caring.&lt;/p&gt;</content:encoded></item></channel></rss>