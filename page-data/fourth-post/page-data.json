{"componentChunkName":"component---src-templates-blog-post-js","path":"/fourth-post/","result":{"data":{"site":{"siteMetadata":{"title":"Notebook"}},"markdownRemark":{"id":"f83517e9-b160-5249-aef8-e05e0414814c","excerpt":"Understanding Text Generation in GPT Models: A Deep Dive In this post, we’ll explore how GPT models generate text step-by-step. By decoding and breaking down…","html":"<h1>Understanding Text Generation in GPT Models: A Deep Dive</h1>\n<p>In this post, we’ll explore how GPT models generate text step-by-step. By decoding and breaking down how tokens are processed, passed through the model, and mapped back to vocabulary, we’ll answer some key questions about how GPT works under the hood.</p>\n<h2>How GPT Generates Text</h2>\n<p>Text generation in GPT models is an <strong>autoregressive process</strong>, which means the model generates one token at a time, based on previously generated tokens. Here’s a simplified function for greedy text generation:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">generate_text_simple</span><span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">,</span> idx<span class=\"token punctuation\">,</span> max_new_tokens<span class=\"token punctuation\">,</span> context_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">for</span> _ <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>max_new_tokens<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token comment\"># Trim the input to fit the model's context window</span>\n        idx_cond <span class=\"token operator\">=</span> idx<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span>context_size<span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span>\n        \n        <span class=\"token comment\"># Pass through the model (disable gradient calculation)</span>\n        <span class=\"token keyword\">with</span> torch<span class=\"token punctuation\">.</span>no_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            logits <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>idx_cond<span class=\"token punctuation\">)</span>\n        \n        <span class=\"token comment\"># Focus on the last time step</span>\n        logits <span class=\"token operator\">=</span> logits<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span>  <span class=\"token comment\"># Shape: (batch_size, vocab_size)</span>\n        \n        <span class=\"token comment\"># Convert logits to probabilities</span>\n        probas <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>softmax<span class=\"token punctuation\">(</span>logits<span class=\"token punctuation\">,</span> dim<span class=\"token operator\">=</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n        \n        <span class=\"token comment\"># Pick the most likely next token (greedy decoding)</span>\n        idx_next <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>argmax<span class=\"token punctuation\">(</span>probas<span class=\"token punctuation\">,</span> dim<span class=\"token operator\">=</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> keepdim<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n        \n        <span class=\"token comment\"># Append the new token to the input sequence</span>\n        idx <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>cat<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>idx<span class=\"token punctuation\">,</span> idx_next<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> dim<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n    \n    <span class=\"token keyword\">return</span> idx</code></pre></div>\n<h3>What’s Happening Here?</h3>\n<p>This function generates new tokens one by one using <strong>greedy decoding</strong>, where we always choose the most likely token at each step. Let’s break it down further:</p>\n<ol>\n<li>\n<p><strong>Trimming the Input Context:</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">idx_cond <span class=\"token operator\">=</span> idx<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span>context_size<span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span></code></pre></div>\n<p>GPT has a fixed context window. If the input sequence exceeds this size, we keep only the most recent tokens.</p>\n</li>\n<li>\n<p><strong>Passing the Input Through the Model:</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">logits <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>idx_cond<span class=\"token punctuation\">)</span></code></pre></div>\n<p>The model processes the input and outputs a set of <strong>logits</strong> for each token in the input sequence. Logits are raw scores that we’ll later convert to probabilities.</p>\n</li>\n<li>\n<p><strong>Extracting the Last Token’s Logits:</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">logits <span class=\"token operator\">=</span> logits<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span>  <span class=\"token comment\"># Shape: (batch_size, vocab_size)</span></code></pre></div>\n<p>Since we’re generating text autoregressively (one token at a time), we only care about the logits for the <strong>last time step</strong>.</p>\n</li>\n<li>\n<p><strong>Converting Logits to Probabilities:</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">probas <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>softmax<span class=\"token punctuation\">(</span>logits<span class=\"token punctuation\">,</span> dim<span class=\"token operator\">=</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>The softmax function normalizes the logits into a <strong>probability distribution</strong> over the entire vocabulary.</p>\n</li>\n<li>\n<p><strong>Selecting the Most Likely Token:</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">idx_next <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>argmax<span class=\"token punctuation\">(</span>probas<span class=\"token punctuation\">,</span> dim<span class=\"token operator\">=</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> keepdim<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>This line picks the token with the highest probability (greedy decoding). The token ID is appended to the input sequence.</p>\n</li>\n</ol>\n<hr>\n<h2>Tokens, Embeddings, and Vocabulary: How It All Fits Together</h2>\n<h3>Step 1: From Token to Embedding</h3>\n<p>When we feed text into GPT, it’s first tokenized into subword units (e.g., “Hello world!” → <code class=\"language-text\">[620, 1567, 198]</code>), and then these token IDs are mapped to <strong>embeddings</strong>. Each embedding is a dense vector of size <code class=\"language-text\">d_model</code> (e.g., 768 for GPT-2 small).</p>\n<h3>Step 2: Passing Embeddings Through Transformer Layers</h3>\n<p>The embeddings are passed through multiple transformer layers, which apply <strong>self-attention</strong>, <strong>feedforward transformations</strong>, and <strong>normalization</strong>. This enriches each token’s embedding with contextual information from the entire input sequence.</p>\n<h3>Step 3: Mapping Hidden States to Logits</h3>\n<p>After the final transformer layer, the hidden states are mapped to a <strong>logits vector</strong> of size <code class=\"language-text\">vocab_size</code>. Each element in this vector represents the model’s score for a particular token in the vocabulary.</p>\n<h3>Step 4: Converting Logits to Tokens</h3>\n<p>To generate the next token:</p>\n<ol>\n<li><strong>Softmax</strong> converts logits to probabilities.</li>\n<li>We pick the most probable token (using greedy decoding, or alternatively, sampling techniques like top-k or nucleus sampling).</li>\n</ol>\n<hr>\n<h2>Why Do We Pick a Token Instead of Directly Converting the Embedding?</h2>\n<p>This is a key insight! The model doesn’t directly output a token—it outputs a high-dimensional vector (logits) with scores for each token in the vocabulary. By selecting the token with the highest score, we pick the <strong>closest match</strong> in terms of meaning and context.</p>\n<hr>\n<h2>Understanding the Vocabulary and Decoding Process</h2>\n<p>The vocabulary is implicitly referenced throughout the process:</p>\n<ul>\n<li><strong>Input tokens (<code class=\"language-text\">idx</code>)</strong> are indices that map to specific vocabulary tokens.</li>\n<li><strong>Model outputs logits</strong> that represent scores for every token in the vocabulary.</li>\n<li><strong>Final decoding step</strong> converts logits to token IDs, which can then be converted back to text using a tokenizer.</li>\n</ul>\n<hr>\n<h2>Visualization: Putting It All Together</h2>\n<p>Here’s a quick visualization of the key steps:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Input Text → Token IDs → Embeddings → Transformer Layers → Logits → Probabilities → Next Token</code></pre></div>\n<hr>\n<h2>Key Takeaways</h2>\n<ol>\n<li><strong>GPT generates text autoregressively</strong>, one token at a time.</li>\n<li><strong>Embeddings are processed through transformer layers</strong>, which enrich them with contextual meaning.</li>\n<li><strong>Logits are mapped to tokens using softmax and decoding methods</strong> (like greedy decoding).</li>\n<li><strong>The vocabulary is referenced implicitly through token IDs</strong>, logits, and final decoding steps.</li>\n</ol>\n<p>By understanding this flow, you’ll gain a deeper appreciation for how GPT models generate coherent and contextually rich text. Whether you’re building your own transformer-based model or fine-tuning GPT, these fundamentals are essential.</p>\n<hr>\n<h2>Next Steps</h2>\n<p>Want to experiment? Try modifying the text generation function to use <strong>top-k sampling</strong>, <strong>temperature scaling</strong>, or <strong>beam search</strong> to explore different decoding strategies!</p>","frontmatter":{"title":"How GPT Models Generate Text: A Step-by-Step Breakdown","date":"February 04, 2025","description":"From token embeddings to decoding strategies, this guide explains how GPT models transform input into coherent text, one token at a time"}}},"pageContext":{"slug":"/fourth-post/","previous":{"fields":{"slug":"/third-post/"},"frontmatter":{"title":"Why Do Neural Networks Need Activation Functions?"}},"next":{"fields":{"slug":"/fifth-post/"},"frontmatter":{"title":"How Stress, Posture, and Breathing Impact Ulcerative Colitis"}}}}}