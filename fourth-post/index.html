<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><style id="typography.js">html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}article,aside,details,figcaption,figure,footer,header,main,menu,nav,section,summary{display:block}audio,canvas,progress,video{display:inline-block}audio:not([controls]){display:none;height:0}progress{vertical-align:baseline}[hidden],template{display:none}a{background-color:transparent;}a:active,a:hover{outline-width:0}abbr[title]{border-bottom:none;text-decoration:underline;text-decoration:underline dotted}b,strong{font-weight:inherit;font-weight:bolder}dfn{font-style:italic}h1{font-size:2em;margin:.67em 0}mark{background-color:#ff0;color:#000}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}img{border-style:none}svg:not(:root){overflow:hidden}code,kbd,pre,samp{font-family:monospace,monospace;font-size:1em}figure{margin:1em 40px}hr{box-sizing:content-box;height:0;overflow:visible}button,input,optgroup,select,textarea{font:inherit;margin:0}optgroup{font-weight:700}button,input{overflow:visible}button,select{text-transform:none}[type=reset],[type=submit],button,html [type=button]{-webkit-appearance:button}[type=button]::-moz-focus-inner,[type=reset]::-moz-focus-inner,[type=submit]::-moz-focus-inner,button::-moz-focus-inner{border-style:none;padding:0}[type=button]:-moz-focusring,[type=reset]:-moz-focusring,[type=submit]:-moz-focusring,button:-moz-focusring{outline:1px dotted ButtonText}fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal}textarea{overflow:auto}[type=checkbox],[type=radio]{box-sizing:border-box;padding:0}[type=number]::-webkit-inner-spin-button,[type=number]::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}[type=search]::-webkit-search-cancel-button,[type=search]::-webkit-search-decoration{-webkit-appearance:none}::-webkit-input-placeholder{color:inherit;opacity:.54}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}html{font:100%/1.75 'Merriweather','Georgia',serif;box-sizing:border-box;overflow-y:scroll;}*{box-sizing:inherit;}*:before{box-sizing:inherit;}*:after{box-sizing:inherit;}body{color:hsla(0,0%,0%,0.9);font-family:'Merriweather','Georgia',serif;font-weight:400;word-wrap:break-word;font-kerning:normal;-moz-font-feature-settings:"kern", "liga", "clig", "calt";-ms-font-feature-settings:"kern", "liga", "clig", "calt";-webkit-font-feature-settings:"kern", "liga", "clig", "calt";font-feature-settings:"kern", "liga", "clig", "calt";}img{max-width:100%;margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.75rem;}h1{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.75rem;color:inherit;font-family:Montserrat,sans-serif;font-weight:900;text-rendering:optimizeLegibility;font-size:2.5rem;line-height:1.1;}h2{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.75rem;color:inherit;font-family:'Merriweather','Georgia',serif;font-weight:900;text-rendering:optimizeLegibility;font-size:1.73286rem;line-height:1.1;}h3{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.75rem;color:inherit;font-family:'Merriweather','Georgia',serif;font-weight:900;text-rendering:optimizeLegibility;font-size:1.4427rem;line-height:1.1;}h4{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.75rem;color:inherit;font-family:'Merriweather','Georgia',serif;font-weight:900;text-rendering:optimizeLegibility;font-size:1rem;line-height:1.1;letter-spacing:0.140625em;text-transform:uppercase;}h5{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.75rem;color:inherit;font-family:'Merriweather','Georgia',serif;font-weight:900;text-rendering:optimizeLegibility;font-size:0.83255rem;line-height:1.1;}h6{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.75rem;color:inherit;font-family:'Merriweather','Georgia',serif;font-weight:900;text-rendering:optimizeLegibility;font-size:0.75966rem;line-height:1.1;font-style:italic;}hgroup{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.75rem;}ul{margin-left:1.75rem;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.75rem;list-style-position:outside;list-style-image:none;list-style:disc;}ol{margin-left:1.75rem;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.75rem;list-style-position:outside;list-style-image:none;}dl{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.75rem;}dd{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.75rem;}p{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.75rem;}figure{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.75rem;}pre{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.75rem;font-size:0.85rem;line-height:1.75rem;}table{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.75rem;font-size:1rem;line-height:1.75rem;border-collapse:collapse;width:100%;}fieldset{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.75rem;}blockquote{margin-left:-1.75rem;margin-right:1.75rem;margin-top:0;padding-bottom:0;padding-left:1.42188rem;padding-right:0;padding-top:0;margin-bottom:1.75rem;font-size:1.20112rem;line-height:1.75rem;color:hsla(0,0%,0%,0.59);font-style:italic;border-left:0.32813rem solid hsla(0,0%,0%,0.9);}form{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.75rem;}noscript{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.75rem;}iframe{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.75rem;}hr{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:calc(1.75rem - 1px);background:hsla(0,0%,0%,0.2);border:none;height:1px;}address{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.75rem;}b{font-weight:700;}strong{font-weight:700;}dt{font-weight:700;}th{font-weight:700;}li{margin-bottom:calc(1.75rem / 2);}ol li{padding-left:0;}ul li{padding-left:0;}li > ol{margin-left:1.75rem;margin-bottom:calc(1.75rem / 2);margin-top:calc(1.75rem / 2);}li > ul{margin-left:1.75rem;margin-bottom:calc(1.75rem / 2);margin-top:calc(1.75rem / 2);}blockquote *:last-child{margin-bottom:0;}li *:last-child{margin-bottom:0;}p *:last-child{margin-bottom:0;}li > p{margin-bottom:calc(1.75rem / 2);}code{font-size:0.85rem;line-height:1.75rem;}kbd{font-size:0.85rem;line-height:1.75rem;}samp{font-size:0.85rem;line-height:1.75rem;}abbr{border-bottom:1px dotted hsla(0,0%,0%,0.5);cursor:help;}acronym{border-bottom:1px dotted hsla(0,0%,0%,0.5);cursor:help;}abbr[title]{border-bottom:1px dotted hsla(0,0%,0%,0.5);cursor:help;text-decoration:none;}thead{text-align:left;}td,th{text-align:left;border-bottom:1px solid hsla(0,0%,0%,0.12);font-feature-settings:"tnum";-moz-font-feature-settings:"tnum";-ms-font-feature-settings:"tnum";-webkit-font-feature-settings:"tnum";padding-left:1.16667rem;padding-right:1.16667rem;padding-top:0.875rem;padding-bottom:calc(0.875rem - 1px);}th:first-child,td:first-child{padding-left:0;}th:last-child,td:last-child{padding-right:0;}blockquote > :last-child{margin-bottom:0;}blockquote cite{font-size:1rem;line-height:1.75rem;color:hsla(0,0%,0%,0.9);font-weight:400;}blockquote cite:before{content:"— ";}ul,ol{margin-left:0;}@media only screen and (max-width:480px){ul,ol{margin-left:1.75rem;}blockquote{margin-left:-1.3125rem;margin-right:0;padding-left:0.98438rem;}}h1,h2,h3,h4,h5,h6{margin-top:2rem;font-family:var(--font-sans);}a{box-shadow:none;color:inherit;text-decoration:none;}a:hover,a:active{box-shadow:none;}mark,ins{background:#007acc;color:white;padding:0.10938rem 0.21875rem;text-decoration:none;}a.gatsby-resp-image-link{box-shadow:none;}</style><meta name="generator" content="Gatsby 5.15.0"/><meta name="theme-color" content="#663399"/><meta data-react-helmet="true" name="description" content="From token embeddings to decoding strategies, this guide explains how GPT models transform input into coherent text, one token at a time"/><meta data-react-helmet="true" property="og:title" content="How GPT Models Generate Text: A Step-by-Step Breakdown"/><meta data-react-helmet="true" property="og:description" content="From token embeddings to decoding strategies, this guide explains how GPT models transform input into coherent text, one token at a time"/><meta data-react-helmet="true" property="og:type" content="website"/><meta data-react-helmet="true" name="twitter:card" content="summary"/><meta data-react-helmet="true" name="twitter:title" content="How GPT Models Generate Text: A Step-by-Step Breakdown"/><meta data-react-helmet="true" name="twitter:description" content="From token embeddings to decoding strategies, this guide explains how GPT models transform input into coherent text, one token at a time"/><style data-href="/styles.140bfa244d7ad8ae6e8a.css" data-identity="gatsby-global-css">code[class*=language-],pre[class*=language-]{word-wrap:normal;background:none;color:#000;font-family:Consolas,Monaco,Andale Mono,Ubuntu Mono,monospace;font-size:1em;-webkit-hyphens:none;hyphens:none;line-height:1.5;tab-size:4;text-align:left;text-shadow:0 1px #fff;white-space:pre;word-break:normal;word-spacing:normal}code[class*=language-] ::selection,code[class*=language-]::selection,pre[class*=language-] ::selection,pre[class*=language-]::selection{background:#b3d4fc;text-shadow:none}@media print{code[class*=language-],pre[class*=language-]{text-shadow:none}}pre[class*=language-]{margin:.5em 0;overflow:auto;padding:1em}:not(pre)>code[class*=language-],pre[class*=language-]{background:#f5f2f0}:not(pre)>code[class*=language-]{border-radius:.3em;padding:.1em;white-space:normal}.token.cdata,.token.comment,.token.doctype,.token.prolog{color:#708090}.token.punctuation{color:#999}.token.namespace{opacity:.7}.token.boolean,.token.constant,.token.deleted,.token.number,.token.property,.token.symbol,.token.tag{color:#905}.token.attr-name,.token.builtin,.token.char,.token.inserted,.token.selector,.token.string{color:#690}.language-css .token.string,.style .token.string,.token.entity,.token.operator,.token.url{background:hsla(0,0%,100%,.5);color:#9a6e3a}.token.atrule,.token.attr-value,.token.keyword{color:#07a}.token.class-name,.token.function{color:#dd4a68}.token.important,.token.regex,.token.variable{color:#e90}.token.bold,.token.important{font-weight:700}.token.italic{font-style:italic}.token.entity{cursor:help}:root{--bg-color:#fff;--text-main:#1a202c;--text-muted:#718096;--accent:#5a67d8;--accent-text:#fff;--grid-line:#f0f4f8;--border-color:#e2e8f0;--font-sans:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,sans-serif;--font-mono:"SFMono-Regular",Consolas,"Liberation Mono",Menlo,Courier,monospace}body{background-color:var(--bg-color);background-image:linear-gradient(90deg,var(--grid-line) 1px,transparent 1px);background-position:50%;background-size:60px 100%;color:var(--text-main)}h1,h2,h3,h4,h5,h6{color:var(--text-main);font-family:var(--font-sans);font-weight:700;letter-spacing:-.025em}.date-tag,nav,small{font-family:var(--font-mono);font-size:.85rem;letter-spacing:.05em}a{border-bottom:1px solid var(--accent);color:var(--text-main);text-decoration:none;transition:all .2s ease}a:hover{background-color:var(--accent);border-bottom-color:transparent;color:var(--accent-text)}.technical-layout{background:#fff;border-left:1px solid var(--border-color);border-right:1px solid var(--border-color);position:relative}.corner-mark{border-color:var(--text-main);border-style:solid;height:12px;pointer-events:none;position:absolute;width:12px}.top-left{border-width:2px 0 0 2px;left:-1px;top:-1px}.top-right{border-width:2px 2px 0 0;right:-1px;top:-1px}.bottom-left{border-width:0 0 2px 2px;bottom:-1px;left:-1px}.bottom-right{border-width:0 2px 2px 0;bottom:-1px;right:-1px}.bracket-header{align-items:center;color:var(--text-muted);display:flex;font-family:var(--font-mono);margin-bottom:2rem;text-transform:uppercase}.bracket-header:before{color:var(--accent);content:"[";font-weight:700;margin-right:.5rem}.bracket-header:after{color:var(--accent);content:"]";font-weight:700;margin-left:.5rem}hr{border:0;border-top:1px dashed var(--border-color);margin:3rem 0}</style><style>.gatsby-image-wrapper{position:relative;overflow:hidden}.gatsby-image-wrapper picture.object-fit-polyfill{position:static!important}.gatsby-image-wrapper img{bottom:0;height:100%;left:0;margin:0;max-width:none;padding:0;position:absolute;right:0;top:0;width:100%;object-fit:cover}.gatsby-image-wrapper [data-main-image]{opacity:0;transform:translateZ(0);transition:opacity .25s linear;will-change:opacity}.gatsby-image-wrapper-constrained{display:inline-block;vertical-align:top}</style><noscript><style>.gatsby-image-wrapper noscript [data-main-image]{opacity:1!important}.gatsby-image-wrapper [data-placeholder-image]{opacity:0!important}</style></noscript><script type="module">const e="undefined"!=typeof HTMLImageElement&&"loading"in HTMLImageElement.prototype;e&&document.body.addEventListener("load",(function(e){const t=e.target;if(void 0===t.dataset.mainImage)return;if(void 0===t.dataset.gatsbyImageSsr)return;let a=null,n=t;for(;null===a&&n;)void 0!==n.parentNode.dataset.gatsbyImageWrapper&&(a=n.parentNode),n=n.parentNode;const o=a.querySelector("[data-placeholder-image]"),r=new Image;r.src=t.currentSrc,r.decode().catch((()=>{})).then((()=>{t.style.opacity=1,o&&(o.style.opacity=0,o.style.transition="opacity 500ms linear")}))}),!0);</script><link rel="preconnect" href="https://www.google-analytics.com"/><link rel="dns-prefetch" href="https://www.google-analytics.com"/><link rel="alternate" type="application/rss+xml" title="Your Site&#x27;s RSS Feed" href="/rss.xml"/><link rel="icon" href="/favicon-32x32.png?v=3c8b24ed2b77f3175e77e9bedfaeb607" type="image/png"/><link rel="icon" href="/favicon.svg?v=3c8b24ed2b77f3175e77e9bedfaeb607" type="image/svg+xml"/><link rel="manifest" href="/manifest.webmanifest" crossorigin="anonymous"/><link rel="apple-touch-icon" sizes="48x48" href="/icons/icon-48x48.png?v=3c8b24ed2b77f3175e77e9bedfaeb607"/><link rel="apple-touch-icon" sizes="72x72" href="/icons/icon-72x72.png?v=3c8b24ed2b77f3175e77e9bedfaeb607"/><link rel="apple-touch-icon" sizes="96x96" href="/icons/icon-96x96.png?v=3c8b24ed2b77f3175e77e9bedfaeb607"/><link rel="apple-touch-icon" sizes="144x144" href="/icons/icon-144x144.png?v=3c8b24ed2b77f3175e77e9bedfaeb607"/><link rel="apple-touch-icon" sizes="192x192" href="/icons/icon-192x192.png?v=3c8b24ed2b77f3175e77e9bedfaeb607"/><link rel="apple-touch-icon" sizes="256x256" href="/icons/icon-256x256.png?v=3c8b24ed2b77f3175e77e9bedfaeb607"/><link rel="apple-touch-icon" sizes="384x384" href="/icons/icon-384x384.png?v=3c8b24ed2b77f3175e77e9bedfaeb607"/><link rel="apple-touch-icon" sizes="512x512" href="/icons/icon-512x512.png?v=3c8b24ed2b77f3175e77e9bedfaeb607"/><title data-react-helmet="true">How GPT Models Generate Text: A Step-by-Step Breakdown | Notebook</title></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><div class="technical-layout" style="margin-left:auto;margin-right:auto;max-width:42rem;padding:0.175rem 1.3125rem;min-height:100vh"><div class="corner-mark top-left"></div><div class="corner-mark top-right"></div><div class="corner-mark bottom-left"></div><div class="corner-mark bottom-right"></div><header style="margin-bottom:1.75rem"><nav style="margin-bottom:0.875rem;margin-top:0.4375rem;font-size:0.85rem;font-family:var(--font-mono);display:flex;justify-content:flex-end"><a style="box-shadow:none;color:var(--text-muted);margin-right:1.75rem;text-decoration:none;border-bottom:none" href="/">[ BLOG ]</a><a style="box-shadow:none;color:var(--text-muted);text-decoration:none;border-bottom:none" href="/links/">[ LINKS ]</a></nav></header><main><article><header><h1 style="margin-top:1.75rem;margin-bottom:0">How GPT Models Generate Text: A Step-by-Step Breakdown</h1><p style="font-size:0.83255rem;line-height:1.75rem;display:block;margin-bottom:1.75rem">February 04, 2025</p></header><section><h1>Understanding Text Generation in GPT Models: A Deep Dive</h1>
<p>In this post, we’ll explore how GPT models generate text step-by-step. By decoding and breaking down how tokens are processed, passed through the model, and mapped back to vocabulary, we’ll answer some key questions about how GPT works under the hood.</p>
<h2>How GPT Generates Text</h2>
<p>Text generation in GPT models is an <strong>autoregressive process</strong>, which means the model generates one token at a time, based on previously generated tokens. Here’s a simplified function for greedy text generation:</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">generate_text_simple</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> idx<span class="token punctuation">,</span> max_new_tokens<span class="token punctuation">,</span> context_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>max_new_tokens<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># Trim the input to fit the model's context window</span>
        idx_cond <span class="token operator">=</span> idx<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span>context_size<span class="token punctuation">:</span><span class="token punctuation">]</span>
        
        <span class="token comment"># Pass through the model (disable gradient calculation)</span>
        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            logits <span class="token operator">=</span> model<span class="token punctuation">(</span>idx_cond<span class="token punctuation">)</span>
        
        <span class="token comment"># Focus on the last time step</span>
        logits <span class="token operator">=</span> logits<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>  <span class="token comment"># Shape: (batch_size, vocab_size)</span>
        
        <span class="token comment"># Convert logits to probabilities</span>
        probas <span class="token operator">=</span> torch<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        
        <span class="token comment"># Pick the most likely next token (greedy decoding)</span>
        idx_next <span class="token operator">=</span> torch<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>probas<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        
        <span class="token comment"># Append the new token to the input sequence</span>
        idx <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>idx<span class="token punctuation">,</span> idx_next<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
    
    <span class="token keyword">return</span> idx</code></pre></div>
<h3>What’s Happening Here?</h3>
<p>This function generates new tokens one by one using <strong>greedy decoding</strong>, where we always choose the most likely token at each step. Let’s break it down further:</p>
<ol>
<li>
<p><strong>Trimming the Input Context:</strong></p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python">idx_cond <span class="token operator">=</span> idx<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span>context_size<span class="token punctuation">:</span><span class="token punctuation">]</span></code></pre></div>
<p>GPT has a fixed context window. If the input sequence exceeds this size, we keep only the most recent tokens.</p>
</li>
<li>
<p><strong>Passing the Input Through the Model:</strong></p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python">logits <span class="token operator">=</span> model<span class="token punctuation">(</span>idx_cond<span class="token punctuation">)</span></code></pre></div>
<p>The model processes the input and outputs a set of <strong>logits</strong> for each token in the input sequence. Logits are raw scores that we’ll later convert to probabilities.</p>
</li>
<li>
<p><strong>Extracting the Last Token’s Logits:</strong></p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python">logits <span class="token operator">=</span> logits<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>  <span class="token comment"># Shape: (batch_size, vocab_size)</span></code></pre></div>
<p>Since we’re generating text autoregressively (one token at a time), we only care about the logits for the <strong>last time step</strong>.</p>
</li>
<li>
<p><strong>Converting Logits to Probabilities:</strong></p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python">probas <span class="token operator">=</span> torch<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span></code></pre></div>
<p>The softmax function normalizes the logits into a <strong>probability distribution</strong> over the entire vocabulary.</p>
</li>
<li>
<p><strong>Selecting the Most Likely Token:</strong></p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python">idx_next <span class="token operator">=</span> torch<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>probas<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></code></pre></div>
<p>This line picks the token with the highest probability (greedy decoding). The token ID is appended to the input sequence.</p>
</li>
</ol>
<hr>
<h2>Tokens, Embeddings, and Vocabulary: How It All Fits Together</h2>
<h3>Step 1: From Token to Embedding</h3>
<p>When we feed text into GPT, it’s first tokenized into subword units (e.g., “Hello world!” → <code class="language-text">[620, 1567, 198]</code>), and then these token IDs are mapped to <strong>embeddings</strong>. Each embedding is a dense vector of size <code class="language-text">d_model</code> (e.g., 768 for GPT-2 small).</p>
<h3>Step 2: Passing Embeddings Through Transformer Layers</h3>
<p>The embeddings are passed through multiple transformer layers, which apply <strong>self-attention</strong>, <strong>feedforward transformations</strong>, and <strong>normalization</strong>. This enriches each token’s embedding with contextual information from the entire input sequence.</p>
<h3>Step 3: Mapping Hidden States to Logits</h3>
<p>After the final transformer layer, the hidden states are mapped to a <strong>logits vector</strong> of size <code class="language-text">vocab_size</code>. Each element in this vector represents the model’s score for a particular token in the vocabulary.</p>
<h3>Step 4: Converting Logits to Tokens</h3>
<p>To generate the next token:</p>
<ol>
<li><strong>Softmax</strong> converts logits to probabilities.</li>
<li>We pick the most probable token (using greedy decoding, or alternatively, sampling techniques like top-k or nucleus sampling).</li>
</ol>
<hr>
<h2>Why Do We Pick a Token Instead of Directly Converting the Embedding?</h2>
<p>This is a key insight! The model doesn’t directly output a token—it outputs a high-dimensional vector (logits) with scores for each token in the vocabulary. By selecting the token with the highest score, we pick the <strong>closest match</strong> in terms of meaning and context.</p>
<hr>
<h2>Understanding the Vocabulary and Decoding Process</h2>
<p>The vocabulary is implicitly referenced throughout the process:</p>
<ul>
<li><strong>Input tokens (<code class="language-text">idx</code>)</strong> are indices that map to specific vocabulary tokens.</li>
<li><strong>Model outputs logits</strong> that represent scores for every token in the vocabulary.</li>
<li><strong>Final decoding step</strong> converts logits to token IDs, which can then be converted back to text using a tokenizer.</li>
</ul>
<hr>
<h2>Visualization: Putting It All Together</h2>
<p>Here’s a quick visualization of the key steps:</p>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">Input Text → Token IDs → Embeddings → Transformer Layers → Logits → Probabilities → Next Token</code></pre></div>
<hr>
<h2>Key Takeaways</h2>
<ol>
<li><strong>GPT generates text autoregressively</strong>, one token at a time.</li>
<li><strong>Embeddings are processed through transformer layers</strong>, which enrich them with contextual meaning.</li>
<li><strong>Logits are mapped to tokens using softmax and decoding methods</strong> (like greedy decoding).</li>
<li><strong>The vocabulary is referenced implicitly through token IDs</strong>, logits, and final decoding steps.</li>
</ol>
<p>By understanding this flow, you’ll gain a deeper appreciation for how GPT models generate coherent and contextually rich text. Whether you’re building your own transformer-based model or fine-tuning GPT, these fundamentals are essential.</p>
<hr>
<h2>Next Steps</h2>
<p>Want to experiment? Try modifying the text generation function to use <strong>top-k sampling</strong>, <strong>temperature scaling</strong>, or <strong>beam search</strong> to explore different decoding strategies!</p></section><hr style="margin-bottom:1.75rem"/><footer><div style="display:flex;margin-bottom:4.375rem"><div data-gatsby-image-wrapper="" style="width:50px;height:50px;margin-right:0.875rem;margin-bottom:0;min-width:50px;border-radius:100%" class="gatsby-image-wrapper"><div aria-hidden="true" data-placeholder-image="" style="opacity:1;transition:opacity 500ms linear;background-color:#080808;width:50px;height:50px;position:relative"></div><picture><source type="image/webp" data-srcset="/static/f7e67e1b7e8e8bd57afb7bc2b2aabd37/dbc4a/profile-pic.webp 50w,/static/f7e67e1b7e8e8bd57afb7bc2b2aabd37/d8057/profile-pic.webp 100w" sizes="50px"/><img data-gatsby-image-ssr="" data-main-image="" style="border-radius:50%;opacity:0" sizes="50px" decoding="async" loading="lazy" data-src="/static/f7e67e1b7e8e8bd57afb7bc2b2aabd37/6ac16/profile-pic.jpg" data-srcset="/static/f7e67e1b7e8e8bd57afb7bc2b2aabd37/6ac16/profile-pic.jpg 50w,/static/f7e67e1b7e8e8bd57afb7bc2b2aabd37/e07e1/profile-pic.jpg 100w" alt="Spenser Filler"/></picture><noscript><picture><source type="image/webp" srcSet="/static/f7e67e1b7e8e8bd57afb7bc2b2aabd37/dbc4a/profile-pic.webp 50w,/static/f7e67e1b7e8e8bd57afb7bc2b2aabd37/d8057/profile-pic.webp 100w" sizes="50px"/><img data-gatsby-image-ssr="" data-main-image="" style="border-radius:50%;opacity:0" sizes="50px" decoding="async" loading="lazy" src="/static/f7e67e1b7e8e8bd57afb7bc2b2aabd37/6ac16/profile-pic.jpg" srcSet="/static/f7e67e1b7e8e8bd57afb7bc2b2aabd37/6ac16/profile-pic.jpg 50w,/static/f7e67e1b7e8e8bd57afb7bc2b2aabd37/e07e1/profile-pic.jpg 100w" alt="Spenser Filler"/></picture></noscript><script type="module">const t="undefined"!=typeof HTMLImageElement&&"loading"in HTMLImageElement.prototype;if(t){const t=document.querySelectorAll("img[data-main-image]");for(let e of t){e.dataset.src&&(e.setAttribute("src",e.dataset.src),e.removeAttribute("data-src")),e.dataset.srcset&&(e.setAttribute("srcset",e.dataset.srcset),e.removeAttribute("data-srcset"));const t=e.parentNode.querySelectorAll("source[data-srcset]");for(let e of t)e.setAttribute("srcset",e.dataset.srcset),e.removeAttribute("data-srcset");e.complete&&(e.style.opacity=1,e.parentNode.parentNode.querySelector("[data-placeholder-image]").style.opacity=0)}}</script></div><p>Written by <strong>Spenser Filler</strong> <!-- -->who thinks.. too much sometimes</p></div></footer></article><nav><ul style="display:flex;flex-wrap:wrap;justify-content:space-between;list-style:none;padding:0"><li><a rel="prev" href="/third-post/">← <!-- -->Why Do Neural Networks Need Activation Functions?</a></li><li><a rel="next" href="/stress-posture-breathing-uc/">How Stress, Posture, and Breathing Impact Ulcerative Colitis<!-- --> →</a></li></ul></nav></main><footer><hr/><span class="date-tag">© <!-- -->2026<!-- -->, Built with<!-- --> <a href="https://www.gatsbyjs.org">Gatsby</a></span></footer></div></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script>
  
  
  if(true) {
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  }
  if (typeof ga === "function") {
    ga('create', 'YOUR_GOOGLE_ANALYTICS_TRACKING_ID', 'auto', {});
      
      
      
      
      
      }</script><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/fourth-post/";/*]]>*/</script><!-- slice-start id="_gatsby-scripts-1" -->
          <script
            id="gatsby-chunk-mapping"
          >
            window.___chunkMapping="{\"app\":[\"/app-5f7ff03af14e04d44616.js\"],\"component---src-pages-404-js\":[\"/component---src-pages-404-js-374e77e746c0d28870e4.js\"],\"component---src-pages-index-js\":[\"/component---src-pages-index-js-cd28bf08aa6f9634ec06.js\"],\"component---src-pages-links-js\":[\"/component---src-pages-links-js-234fec208dad2f1b5dfc.js\"],\"component---src-templates-blog-post-js\":[\"/component---src-templates-blog-post-js-ec6e0618f649e15132b7.js\"]}";
          </script>
        <script>window.___webpackCompilationHash="9d42aee3a66ebcf59dc9";</script><script src="/webpack-runtime-2e4af0b63dd262fd5364.js" async></script><script src="/framework-59d39cfa83f932efc814.js" async></script><script src="/app-5f7ff03af14e04d44616.js" async></script><!-- slice-end id="_gatsby-scripts-1" --></body></html>